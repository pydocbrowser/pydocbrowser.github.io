<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
          "DTD/xhtml1-strict.dtd">
<html>
  
  
  <head>
    
    <title>Class Hierarchy</title>
    <meta name="generator" content="pydoctor 22.9.1"> 
        
    </meta>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1 maximum-scale=1" />
    <link rel="stylesheet" type="text/css" href="bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="apidocs.css" />
    <link rel="stylesheet" type="text/css" href="extra.css" />
</head>

  <body>

    <div id="banner">    
    <div>
        <a href="/">Home</a>
        &gt; scrapy-2.7.0 <!-- This is a placeholder -->
        
        <!-- (<a href=""&gt;show all versions</a&gt;) -->
    </div>
</div>

    <nav class="navbar navbar-default mainnavbar">
      
  
  <div class="container-fluid">


    <div class="navbar-header">
      
      <div class="navlinks">
        <span class="navbar-brand">
          scrapy <a href="index.html">API Documentation</a>
        </span>

        <a href="moduleIndex.html">
          Modules
        </a>

        <a href="classIndex.html">
          Classes
        </a>

        <a href="nameIndex.html">
          Names
        </a>

        <div id="search-box-container">
          <div class="input-group">
            <input id="search-box" type="search" name="search-query" placeholder="Search..." aria-label="Search" minlength="2" class="form-control" autocomplete="off" />
            
            <span class="input-group-btn">
              <a style="display: none;" class="btn btn-default" id="search-clear-button" title="Clear" onclick="clearSearch()"><img src="fonts/x-circle.svg" alt="Clear" /></a>
              <a class="btn btn-default" id="search-help-button" title="Help" onclick="toggleSearchHelpText()"><img src="fonts/info.svg" alt="Help" /></a>
            </span>
          </div>
        </div>

      </div>

      <div id="search-results-container" style="display: none;">
        
        <div id="search-buttons">
          <span class="label label-default" id="search-docstrings-button">
          <label class="checkbox-inline">
            <input type="checkbox" id="toggle-search-in-docstrings-checkbox" value="false" onclick="toggleSearchInDocstrings()">
              search in docstrings
            </input>
          </label>
          </span>
          
        </div>
        
        <noscript>
            <h1>Cannot search: JavaScript is not supported/enabled in your browser.</h1>
        </noscript>

        <div class="hint" id="search-help-box">
          <p class="rst-last">
      
            Search bar offers the following options:
            <ul>   
                <li>
                  <strong>Term presence.</strong> The below example searches for documents that 
                    must contain “foo”, might contain “bar” and must not contain “baz”: <code>+foo bar -baz</code>
                </li> 

                <li>
                  <strong>Wildcards.</strong> The below example searches for documents with words beginning with “foo”: <code>foo*</code>
                </li> 

                <li>
                  <strong>Search in specific fields.</strong> The following search matches all objects 
                  in "twisted.mail" that matches “search”: <code>+qname:twisted.mail.* +search</code>

                  <p>
                    Possible fields: 'name', 'qname' (fully qualified name), 'docstring', and 'kind'.
                    Last two fields are only applicable if "search in docstrings" is enabled.
                  </p>
                </li>

                <li>
                  <strong>Fuzzy matches.</strong> The following search matches all documents 
                  that have a word within 1 edit distance of “foo”: <code>foo~1</code>
                </li>
            </ul>

          </p>
        </div>

        <div id="search-status"> </div>

        <div class="warning" id="search-warn-box" style="display: none;">
          <p class="rst-last"><span id="search-warn"></span></p>
        </div>

        <table id="search-results">
          <!-- Filled dynamically by JS -->
        </table>
        
        <div style="margin-top: 8px;">
          <p>Results provided by <a href="https://lunrjs.com">Lunr.js</a></p>
        </div>
      </div>

    </div>


  </div>
</nav>

    <div class="container-fluid">

      <div class="page-header">
        <h1>Class Hierarchy</h1>
        <div id="showPrivate">
          <button class="btn btn-link" onclick="togglePrivate()">Toggle Private API</button>
        </div>
      </div>

      <ul id="summaryTree">
      <li><code>abc.ABCMeta</code><ul><li><a name="scrapy.item.ItemMeta"></a><code><a href="scrapy.item.ItemMeta.html" class="internal-link">scrapy.item.ItemMeta</a></code> - Metaclass_ of :class:`Item` that handles field definitions.</li></ul></li><li><code>argparse.ArgumentParser</code><ul><li><a name="scrapy.cmdline.ScrapyArgumentParser"></a><code><a href="scrapy.cmdline.ScrapyArgumentParser.html" class="internal-link">scrapy.cmdline.ScrapyArgumentParser</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.utils.curl.CurlParser"></a><code><a href="scrapy.utils.curl.CurlParser.html" class="internal-link">scrapy.utils.curl.CurlParser</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><code>argparse.HelpFormatter</code><ul><li><a name="scrapy.commands.ScrapyHelpFormatter"></a><code><a href="scrapy.commands.ScrapyHelpFormatter.html" class="internal-link">scrapy.commands.ScrapyHelpFormatter</a></code> - Help Formatter for scrapy command line help messages.</li></ul></li><li><code>AssertionError</code><ul><li><a name="scrapy.exceptions.ContractFail"></a><code><a href="scrapy.exceptions.ContractFail.html" class="internal-link">scrapy.exceptions.ContractFail</a></code> - Error raised in case of a failing contract</li></ul></li><li><code>collections.abc.MutableMapping</code><ul><li><a name="scrapy.Item"></a><code><a href="scrapy.Item.html" class="internal-link">scrapy.Item</a></code> - Base class for scraped items.</li><li class="private"><a name="scrapy.settings._DictProxy"></a><code><a href="scrapy.settings._DictProxy.html" class="internal-link">scrapy.settings._DictProxy</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.settings.BaseSettings"></a><code><a href="scrapy.settings.BaseSettings.html" class="internal-link">scrapy.settings.BaseSettings</a></code> - Instances of this class behave like dictionaries, but store priorities along with their ``(key, value)`` pairs, and can be frozen (i.e. marked immutable).<ul><li><a name="scrapy.settings.Settings"></a><code><a href="scrapy.settings.Settings.html" class="internal-link">scrapy.settings.Settings</a></code> - This object stores Scrapy settings for the configuration of internal components, and can be used for any further customization.</li></ul></li></ul></li><li><code>collections.OrderedDict</code><ul><li><a name="scrapy.utils.datatypes.LocalCache"></a><code><a href="scrapy.utils.datatypes.LocalCache.html" class="internal-link">scrapy.utils.datatypes.LocalCache</a></code> - Dictionary with a finite number of keys.</li></ul></li><li><code>dict</code><ul><li><a name="scrapy.Field"></a><code><a href="scrapy.Field.html" class="internal-link">scrapy.Field</a></code> - Container of field metadata</li><li><a name="scrapy.utils.datatypes.CaselessDict"></a><code><a href="scrapy.utils.datatypes.CaselessDict.html" class="internal-link">scrapy.utils.datatypes.CaselessDict</a></code> - <span class="undocumented">No class docstring; 0/1 class variable, 2/12 methods, 0/1 class method documented</span><ul><li><a name="scrapy.http.headers.Headers"></a><code><a href="scrapy.http.headers.Headers.html" class="internal-link">scrapy.http.headers.Headers</a></code> - Case insensitive http headers dictionary</li></ul></li></ul></li><li><code>enum.Enum</code><ul><li><a name="scrapy.core.http2.stream.StreamCloseReason"></a><code><a href="scrapy.core.http2.stream.StreamCloseReason.html" class="internal-link">scrapy.core.http2.stream.StreamCloseReason</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><code>Exception</code><ul><li><a name="scrapy.core.downloader.handlers.http11.TunnelError"></a><code><a href="scrapy.core.downloader.handlers.http11.TunnelError.html" class="internal-link">scrapy.core.downloader.handlers.http11.TunnelError</a></code> - An HTTP CONNECT tunnel could not be established by the proxy.</li><li><a name="scrapy.exceptions.CloseSpider"></a><code><a href="scrapy.exceptions.CloseSpider.html" class="internal-link">scrapy.exceptions.CloseSpider</a></code> - Raise this from callbacks to request the spider to be closed</li><li><a name="scrapy.exceptions.DontCloseSpider"></a><code><a href="scrapy.exceptions.DontCloseSpider.html" class="internal-link">scrapy.exceptions.DontCloseSpider</a></code> - Request the spider not to be closed yet</li><li><a name="scrapy.exceptions.DropItem"></a><code><a href="scrapy.exceptions.DropItem.html" class="internal-link">scrapy.exceptions.DropItem</a></code> - Drop item from the item pipeline<ul><li><a name="scrapy.pipelines.images.NoimagesDrop"></a><code><a href="scrapy.pipelines.images.NoimagesDrop.html" class="internal-link">scrapy.pipelines.images.NoimagesDrop</a></code> - Product with no images exception</li></ul></li><li><a name="scrapy.exceptions.IgnoreRequest"></a><code><a href="scrapy.exceptions.IgnoreRequest.html" class="internal-link">scrapy.exceptions.IgnoreRequest</a></code> - Indicates a decision was made not to process a request<ul><li><a name="scrapy.spidermiddlewares.httperror.HttpError"></a><code><a href="scrapy.spidermiddlewares.httperror.HttpError.html" class="internal-link">scrapy.spidermiddlewares.httperror.HttpError</a></code> - A non-200 response was filtered</li></ul></li><li><a name="scrapy.exceptions.NotConfigured"></a><code><a href="scrapy.exceptions.NotConfigured.html" class="internal-link">scrapy.exceptions.NotConfigured</a></code> - Indicates a missing configuration situation</li><li><a name="scrapy.exceptions.NotSupported"></a><code><a href="scrapy.exceptions.NotSupported.html" class="internal-link">scrapy.exceptions.NotSupported</a></code> - Indicates a feature or method is not supported</li><li><a name="scrapy.exceptions.StopDownload"></a><code><a href="scrapy.exceptions.StopDownload.html" class="internal-link">scrapy.exceptions.StopDownload</a></code> - Stop the download of the body for a given response. The 'fail' boolean parameter indicates whether or not the resulting partial response should be handled by the request errback. Note that 'fail' is a keyword-only argument.</li><li><a name="scrapy.exceptions.UsageError"></a><code><a href="scrapy.exceptions.UsageError.html" class="internal-link">scrapy.exceptions.UsageError</a></code> - To indicate a command-line usage error</li><li><a name="scrapy.pipelines.files.FileException"></a><code><a href="scrapy.pipelines.files.FileException.html" class="internal-link">scrapy.pipelines.files.FileException</a></code> - General media error exception<ul><li><a name="scrapy.pipelines.images.ImageException"></a><code><a href="scrapy.pipelines.images.ImageException.html" class="internal-link">scrapy.pipelines.images.ImageException</a></code> - General image error exception</li></ul></li></ul></li><li><code>h2.exceptions.H2Error</code><ul><li><a name="scrapy.core.http2.protocol.InvalidNegotiatedProtocol"></a><code><a href="scrapy.core.http2.protocol.InvalidNegotiatedProtocol.html" class="internal-link">scrapy.core.http2.protocol.InvalidNegotiatedProtocol</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.core.http2.protocol.MethodNotAllowed405"></a><code><a href="scrapy.core.http2.protocol.MethodNotAllowed405.html" class="internal-link">scrapy.core.http2.protocol.MethodNotAllowed405</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.core.http2.protocol.RemoteTerminatedConnection"></a><code><a href="scrapy.core.http2.protocol.RemoteTerminatedConnection.html" class="internal-link">scrapy.core.http2.protocol.RemoteTerminatedConnection</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.core.http2.stream.InvalidHostname"></a><code><a href="scrapy.core.http2.stream.InvalidHostname.html" class="internal-link">scrapy.core.http2.stream.InvalidHostname</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><code>io.IOBase</code><ul><li><a name="scrapy.extensions.postprocessing.PostProcessingManager"></a><code><a href="scrapy.extensions.postprocessing.PostProcessingManager.html" class="internal-link">scrapy.extensions.postprocessing.PostProcessingManager</a></code> - This will manage and use declared plugins to process data in a pipeline-ish way. :param plugins: all the declared plugins for the feed :type plugins: list :param file: final target file where the processed data will be written :type file: file like object...</li></ul></li><li><code>itemloaders.ItemLoader</code><ul><li><a name="scrapy.loader.ItemLoader"></a><code><a href="scrapy.loader.ItemLoader.html" class="internal-link">scrapy.loader.ItemLoader</a></code> - A user-friendly abstraction to populate an :ref:`item &lt;topics-items&gt;` with data by applying :ref:`field processors &lt;topics-loaders-processors&gt;` to scraped data. When instantiated with a ``selector`` or a ``response`` it supports data extraction from web pages using :ref:`selectors &lt;topics-selectors&gt;`.</li></ul></li><li><code>json.JSONDecoder</code><ul><li><a name="scrapy.utils.serialize.ScrapyJSONDecoder"></a><code><a href="scrapy.utils.serialize.ScrapyJSONDecoder.html" class="internal-link">scrapy.utils.serialize.ScrapyJSONDecoder</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><code>json.JSONEncoder</code><ul><li><a name="scrapy.utils.serialize.ScrapyJSONEncoder"></a><code><a href="scrapy.utils.serialize.ScrapyJSONEncoder.html" class="internal-link">scrapy.utils.serialize.ScrapyJSONEncoder</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><code>logging.Filter</code><ul><li><a name="scrapy.utils.log.TopLevelFormatter"></a><code><a href="scrapy.utils.log.TopLevelFormatter.html" class="internal-link">scrapy.utils.log.TopLevelFormatter</a></code> - Keep only top level loggers's name (direct children from root) from records.</li></ul></li><li><code>logging.Handler</code><ul><li><a name="scrapy.utils.log.LogCounterHandler"></a><code><a href="scrapy.utils.log.LogCounterHandler.html" class="internal-link">scrapy.utils.log.LogCounterHandler</a></code> - Record log levels count into a crawler stats</li></ul></li><li><code>parsel.Selector</code><ul><li><a name="scrapy.Selector"></a><code><a href="scrapy.Selector.html" class="internal-link">scrapy.Selector</a></code> - An instance of :class:`Selector` is a wrapper over response to select certain parts of its content.</li></ul></li><li><code>parsel.Selector.selectorlist_cls</code><ul><li><a name="scrapy.selector.unified.SelectorList"></a><code><a href="scrapy.selector.unified.SelectorList.html" class="internal-link">scrapy.selector.unified.SelectorList</a></code> - The :class:`SelectorList` class is a subclass of the builtin ``list`` class, which provides a few additional methods.</li></ul></li><li class="private"><a name="scrapy.commands.bench._BenchServer"></a><code><a href="scrapy.commands.bench._BenchServer.html" class="internal-link">scrapy.commands.bench._BenchServer</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.commands.ScrapyCommand"></a><code><a href="scrapy.commands.ScrapyCommand.html" class="internal-link">scrapy.commands.ScrapyCommand</a></code> - <span class="undocumented">No class docstring; 0/2 instance variable, 0/4 class variable, 6/9 methods documented</span><ul><li><a name="scrapy.commands.BaseRunSpiderCommand"></a><code><a href="scrapy.commands.BaseRunSpiderCommand.html" class="internal-link">scrapy.commands.BaseRunSpiderCommand</a></code> - Common class used to share functionality between the crawl, parse and runspider commands<ul><li><a name="scrapy.commands.crawl.Command"></a><code><a href="scrapy.commands.crawl.Command.html" class="internal-link">scrapy.commands.crawl.Command</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.commands.parse.Command"></a><code><a href="scrapy.commands.parse.Command.html" class="internal-link">scrapy.commands.parse.Command</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.commands.runspider.Command"></a><code><a href="scrapy.commands.runspider.Command.html" class="internal-link">scrapy.commands.runspider.Command</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><a name="scrapy.commands.bench.Command"></a><code><a href="scrapy.commands.bench.Command.html" class="internal-link">scrapy.commands.bench.Command</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.commands.check.Command"></a><code><a href="scrapy.commands.check.Command.html" class="internal-link">scrapy.commands.check.Command</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.commands.edit.Command"></a><code><a href="scrapy.commands.edit.Command.html" class="internal-link">scrapy.commands.edit.Command</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.commands.fetch.Command"></a><code><a href="scrapy.commands.fetch.Command.html" class="internal-link">scrapy.commands.fetch.Command</a></code> - <span class="undocumented">Undocumented</span><ul><li><a name="scrapy.commands.view.Command"></a><code><a href="scrapy.commands.view.Command.html" class="internal-link">scrapy.commands.view.Command</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><a name="scrapy.commands.genspider.Command"></a><code><a href="scrapy.commands.genspider.Command.html" class="internal-link">scrapy.commands.genspider.Command</a></code> - <span class="undocumented">No class docstring; 0/1 property, 0/1 instance variable, 0/2 class variable, 1/8 method documented</span></li><li><a name="scrapy.commands.list.Command"></a><code><a href="scrapy.commands.list.Command.html" class="internal-link">scrapy.commands.list.Command</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.commands.settings.Command"></a><code><a href="scrapy.commands.settings.Command.html" class="internal-link">scrapy.commands.settings.Command</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.commands.shell.Command"></a><code><a href="scrapy.commands.shell.Command.html" class="internal-link">scrapy.commands.shell.Command</a></code> - <span class="undocumented">No class docstring; 0/2 class variable, 1/7 method documented</span></li><li><a name="scrapy.commands.startproject.Command"></a><code><a href="scrapy.commands.startproject.Command.html" class="internal-link">scrapy.commands.startproject.Command</a></code> - <span class="undocumented">No class docstring; 0/1 property, 0/1 instance variable, 0/2 class variable, 1/5 method documented</span></li><li><a name="scrapy.commands.version.Command"></a><code><a href="scrapy.commands.version.Command.html" class="internal-link">scrapy.commands.version.Command</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><a name="scrapy.contracts.Contract"></a><code><a href="scrapy.contracts.Contract.html" class="internal-link">scrapy.contracts.Contract</a></code> - Abstract class for contracts <ul><li><a name="scrapy.contracts.default.CallbackKeywordArgumentsContract"></a><code><a href="scrapy.contracts.default.CallbackKeywordArgumentsContract.html" class="internal-link">scrapy.contracts.default.CallbackKeywordArgumentsContract</a></code> - Contract to set the keyword arguments for the request. The value should be a JSON-encoded dictionary, e.g.:</li><li><a name="scrapy.contracts.default.ReturnsContract"></a><code><a href="scrapy.contracts.default.ReturnsContract.html" class="internal-link">scrapy.contracts.default.ReturnsContract</a></code> - Contract to check the output of a callback</li><li><a name="scrapy.contracts.default.ScrapesContract"></a><code><a href="scrapy.contracts.default.ScrapesContract.html" class="internal-link">scrapy.contracts.default.ScrapesContract</a></code> - Contract to check presence of fields in scraped items @scrapes page_name page_body</li><li><a name="scrapy.contracts.default.UrlContract"></a><code><a href="scrapy.contracts.default.UrlContract.html" class="internal-link">scrapy.contracts.default.UrlContract</a></code> - Contract to set the url of the request (mandatory) @url http://scrapy.org</li></ul></li><li><a name="scrapy.contracts.ContractsManager"></a><code><a href="scrapy.contracts.ContractsManager.html" class="internal-link">scrapy.contracts.ContractsManager</a></code> - <span class="undocumented">No class docstring; 0/1 class variable, 1/6 method documented</span></li><li><a name="scrapy.core.downloader.contextfactory.AcceptableProtocolsContextFactory"></a><code><a href="scrapy.core.downloader.contextfactory.AcceptableProtocolsContextFactory.html" class="internal-link">scrapy.core.downloader.contextfactory.AcceptableProtocolsContextFactory</a></code> - Context factory to used to override the acceptable protocols to set up the [OpenSSL.SSL.Context] for doing NPN and/or ALPN negotiation.</li><li><a name="scrapy.core.downloader.Downloader"></a><code><a href="scrapy.core.downloader.Downloader.html" class="internal-link">scrapy.core.downloader.Downloader</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler"></a><code><a href="scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler.html" class="internal-link">scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.core.downloader.handlers.DownloadHandlers"></a><code><a href="scrapy.core.downloader.handlers.DownloadHandlers.html" class="internal-link">scrapy.core.downloader.handlers.DownloadHandlers</a></code> - <span class="undocumented">No class docstring; 0/4 instance variable, 1/5 method documented</span></li><li><a name="scrapy.core.downloader.handlers.file.FileDownloadHandler"></a><code><a href="scrapy.core.downloader.handlers.file.FileDownloadHandler.html" class="internal-link">scrapy.core.downloader.handlers.file.FileDownloadHandler</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.core.downloader.handlers.ftp.FTPDownloadHandler"></a><code><a href="scrapy.core.downloader.handlers.ftp.FTPDownloadHandler.html" class="internal-link">scrapy.core.downloader.handlers.ftp.FTPDownloadHandler</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler"></a><code><a href="scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler.html" class="internal-link">scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler</a></code> - <span class="undocumented">No class docstring; 0/4 instance variable, 0/1 class variable, 1/3 method, 0/1 class method documented</span></li><li class="private"><a name="scrapy.core.downloader.handlers.http11._RequestBodyProducer"></a><code><a href="scrapy.core.downloader.handlers.http11._RequestBodyProducer.html" class="internal-link">scrapy.core.downloader.handlers.http11._RequestBodyProducer</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler"></a><code><a href="scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.html" class="internal-link">scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler</a></code> - <span class="undocumented">No class docstring; 0/7 instance variable, 0/1 class variable, 1/3 method, 0/1 class method documented</span></li><li><a name="scrapy.core.downloader.handlers.http11.ScrapyAgent"></a><code><a href="scrapy.core.downloader.handlers.http11.ScrapyAgent.html" class="internal-link">scrapy.core.downloader.handlers.http11.ScrapyAgent</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.core.downloader.handlers.http2.H2DownloadHandler"></a><code><a href="scrapy.core.downloader.handlers.http2.H2DownloadHandler.html" class="internal-link">scrapy.core.downloader.handlers.http2.H2DownloadHandler</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.core.downloader.handlers.http2.ScrapyH2Agent"></a><code><a href="scrapy.core.downloader.handlers.http2.ScrapyH2Agent.html" class="internal-link">scrapy.core.downloader.handlers.http2.ScrapyH2Agent</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.core.downloader.handlers.s3.S3DownloadHandler"></a><code><a href="scrapy.core.downloader.handlers.s3.S3DownloadHandler.html" class="internal-link">scrapy.core.downloader.handlers.s3.S3DownloadHandler</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.core.downloader.Slot"></a><code><a href="scrapy.core.downloader.Slot.html" class="internal-link">scrapy.core.downloader.Slot</a></code> - Downloader slot</li><li><a name="scrapy.core.engine.ExecutionEngine"></a><code><a href="scrapy.core.engine.ExecutionEngine.html" class="internal-link">scrapy.core.engine.ExecutionEngine</a></code> - <span class="undocumented">No class docstring; 0/1 property, 0/14 instance variable, 6/22 methods documented</span></li><li><a name="scrapy.core.engine.Slot"></a><code><a href="scrapy.core.engine.Slot.html" class="internal-link">scrapy.core.engine.Slot</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.core.http2.agent.H2Agent"></a><code><a href="scrapy.core.http2.agent.H2Agent.html" class="internal-link">scrapy.core.http2.agent.H2Agent</a></code> - <span class="undocumented">No class docstring; 0/4 instance variable, 1/4 method documented</span><ul><li><a name="scrapy.core.http2.agent.ScrapyProxyH2Agent"></a><code><a href="scrapy.core.http2.agent.ScrapyProxyH2Agent.html" class="internal-link">scrapy.core.http2.agent.ScrapyProxyH2Agent</a></code> - <span class="undocumented">No class docstring; 0/1 instance variable, 1/3 method documented</span></li></ul></li><li><a name="scrapy.core.http2.agent.H2ConnectionPool"></a><code><a href="scrapy.core.http2.agent.H2ConnectionPool.html" class="internal-link">scrapy.core.http2.agent.H2ConnectionPool</a></code> - <span class="undocumented">No class docstring; 0/4 instance variable, 1/6 method documented</span></li><li><a name="scrapy.core.http2.stream.Stream"></a><code><a href="scrapy.core.http2.stream.Stream.html" class="internal-link">scrapy.core.http2.stream.Stream</a></code> - Represents a single HTTP/2 Stream.</li><li><a name="scrapy.core.scheduler.BaseScheduler"></a><code><a href="scrapy.core.scheduler.BaseScheduler.html" class="internal-link">scrapy.core.scheduler.BaseScheduler</a></code> - The scheduler component is responsible for storing requests received from the engine, and feeding them back upon request (also to the engine).<ul><li><a name="scrapy.core.scheduler.Scheduler"></a><code><a href="scrapy.core.scheduler.Scheduler.html" class="internal-link">scrapy.core.scheduler.Scheduler</a></code> - Default Scrapy scheduler. This implementation also handles duplication filtering via the :setting:`dupefilter &lt;DUPEFILTER_CLASS&gt;`.</li></ul></li><li><a name="scrapy.core.scraper.Scraper"></a><code><a href="scrapy.core.scraper.Scraper.html" class="internal-link">scrapy.core.scraper.Scraper</a></code> - <span class="undocumented">No class docstring; 0/7 instance variable, 8/15 methods documented</span></li><li><a name="scrapy.core.scraper.Slot"></a><code><a href="scrapy.core.scraper.Slot.html" class="internal-link">scrapy.core.scraper.Slot</a></code> - Scraper slot (one per running spider)</li><li><a name="scrapy.crawler.Crawler"></a><code><a href="scrapy.crawler.Crawler.html" class="internal-link">scrapy.crawler.Crawler</a></code> - <span class="undocumented">No class docstring; 0/11 instance variable, 1/5 method documented</span></li><li><a name="scrapy.crawler.CrawlerRunner"></a><code><a href="scrapy.crawler.CrawlerRunner.html" class="internal-link">scrapy.crawler.CrawlerRunner</a></code> - This is a convenient helper class that keeps track of, manages and runs crawlers inside an already setup :mod:`~twisted.internet.reactor`.<ul><li><a name="scrapy.crawler.CrawlerProcess"></a><code><a href="scrapy.crawler.CrawlerProcess.html" class="internal-link">scrapy.crawler.CrawlerProcess</a></code> - A class to run multiple scrapy crawlers in a process simultaneously.</li></ul></li><li><a name="scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware"></a><code><a href="scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware.html" class="internal-link">scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware</a></code> - Handle 'AJAX crawlable' pages marked as crawlable via meta tag. For more info see https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.</li><li><a name="scrapy.downloadermiddlewares.cookies.CookiesMiddleware"></a><code><a href="scrapy.downloadermiddlewares.cookies.CookiesMiddleware.html" class="internal-link">scrapy.downloadermiddlewares.cookies.CookiesMiddleware</a></code> - This middleware enables working with sites that need cookies</li><li><a name="scrapy.downloadermiddlewares.decompression.DecompressionMiddleware"></a><code><a href="scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.html" class="internal-link">scrapy.downloadermiddlewares.decompression.DecompressionMiddleware</a></code> - This middleware tries to recognise and extract the possibly compressed responses that may arrive. </li><li><a name="scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware"></a><code><a href="scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware.html" class="internal-link">scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware"></a><code><a href="scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.html" class="internal-link">scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"></a><code><a href="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware.html" class="internal-link">scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware</a></code> - Set Basic HTTP Authorization header (http_user and http_pass spider class attributes)</li><li><a name="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware"></a><code><a href="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.html" class="internal-link">scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"></a><code><a href="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware.html" class="internal-link">scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware</a></code> - This middleware allows compressed (gzip, deflate) traffic to be sent/received from web sites</li><li><a name="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"></a><code><a href="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware.html" class="internal-link">scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware"></a><code><a href="scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware.html" class="internal-link">scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware</a></code> - <span class="undocumented">Undocumented</span><ul><li><a name="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware"></a><code><a href="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware.html" class="internal-link">scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.downloadermiddlewares.redirect.RedirectMiddleware"></a><code><a href="scrapy.downloadermiddlewares.redirect.RedirectMiddleware.html" class="internal-link">scrapy.downloadermiddlewares.redirect.RedirectMiddleware</a></code> - Handle redirection of requests based on response status and meta-refresh html tag.</li></ul></li><li><a name="scrapy.downloadermiddlewares.retry.RetryMiddleware"></a><code><a href="scrapy.downloadermiddlewares.retry.RetryMiddleware.html" class="internal-link">scrapy.downloadermiddlewares.retry.RetryMiddleware</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware"></a><code><a href="scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.html" class="internal-link">scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.downloadermiddlewares.stats.DownloaderStats"></a><code><a href="scrapy.downloadermiddlewares.stats.DownloaderStats.html" class="internal-link">scrapy.downloadermiddlewares.stats.DownloaderStats</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.downloadermiddlewares.useragent.UserAgentMiddleware"></a><code><a href="scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.html" class="internal-link">scrapy.downloadermiddlewares.useragent.UserAgentMiddleware</a></code> - This middleware allows spiders to override the user_agent</li><li><a name="scrapy.dupefilters.BaseDupeFilter"></a><code><a href="scrapy.dupefilters.BaseDupeFilter.html" class="internal-link">scrapy.dupefilters.BaseDupeFilter</a></code> - <span class="undocumented">No class docstring; 1/4 method, 0/1 class method documented</span><ul><li><a name="scrapy.dupefilters.RFPDupeFilter"></a><code><a href="scrapy.dupefilters.RFPDupeFilter.html" class="internal-link">scrapy.dupefilters.RFPDupeFilter</a></code> - Request Fingerprint duplicates filter</li></ul></li><li><a name="scrapy.exporters.BaseItemExporter"></a><code><a href="scrapy.exporters.BaseItemExporter.html" class="internal-link">scrapy.exporters.BaseItemExporter</a></code> - <span class="undocumented">No class docstring; 0/5 instance variable, 2/7 methods documented</span><ul><li><a name="scrapy.exporters.CsvItemExporter"></a><code><a href="scrapy.exporters.CsvItemExporter.html" class="internal-link">scrapy.exporters.CsvItemExporter</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.exporters.JsonItemExporter"></a><code><a href="scrapy.exporters.JsonItemExporter.html" class="internal-link">scrapy.exporters.JsonItemExporter</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.exporters.JsonLinesItemExporter"></a><code><a href="scrapy.exporters.JsonLinesItemExporter.html" class="internal-link">scrapy.exporters.JsonLinesItemExporter</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.exporters.MarshalItemExporter"></a><code><a href="scrapy.exporters.MarshalItemExporter.html" class="internal-link">scrapy.exporters.MarshalItemExporter</a></code> - Exports items in a Python-specific binary format (see :mod:`marshal`).</li><li><a name="scrapy.exporters.PickleItemExporter"></a><code><a href="scrapy.exporters.PickleItemExporter.html" class="internal-link">scrapy.exporters.PickleItemExporter</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.exporters.PprintItemExporter"></a><code><a href="scrapy.exporters.PprintItemExporter.html" class="internal-link">scrapy.exporters.PprintItemExporter</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.exporters.PythonItemExporter"></a><code><a href="scrapy.exporters.PythonItemExporter.html" class="internal-link">scrapy.exporters.PythonItemExporter</a></code> - This is a base class for item exporters that extends :class:`BaseItemExporter` with support for nested items.</li><li><a name="scrapy.exporters.XmlItemExporter"></a><code><a href="scrapy.exporters.XmlItemExporter.html" class="internal-link">scrapy.exporters.XmlItemExporter</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><a name="scrapy.extensions.closespider.CloseSpider"></a><code><a href="scrapy.extensions.closespider.CloseSpider.html" class="internal-link">scrapy.extensions.closespider.CloseSpider</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.extensions.corestats.CoreStats"></a><code><a href="scrapy.extensions.corestats.CoreStats.html" class="internal-link">scrapy.extensions.corestats.CoreStats</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.extensions.debug.Debugger"></a><code><a href="scrapy.extensions.debug.Debugger.html" class="internal-link">scrapy.extensions.debug.Debugger</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.extensions.debug.StackTraceDump"></a><code><a href="scrapy.extensions.debug.StackTraceDump.html" class="internal-link">scrapy.extensions.debug.StackTraceDump</a></code> - <span class="undocumented">Undocumented</span></li><li class="private"><a name="scrapy.extensions.feedexport._FeedSlot"></a><code><a href="scrapy.extensions.feedexport._FeedSlot.html" class="internal-link">scrapy.extensions.feedexport._FeedSlot</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.extensions.feedexport.BlockingFeedStorage"></a><code><a href="scrapy.extensions.feedexport.BlockingFeedStorage.html" class="internal-link">scrapy.extensions.feedexport.BlockingFeedStorage</a></code> - <span class="undocumented">Undocumented</span><ul><li><a name="scrapy.extensions.feedexport.FTPFeedStorage"></a><code><a href="scrapy.extensions.feedexport.FTPFeedStorage.html" class="internal-link">scrapy.extensions.feedexport.FTPFeedStorage</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.extensions.feedexport.GCSFeedStorage"></a><code><a href="scrapy.extensions.feedexport.GCSFeedStorage.html" class="internal-link">scrapy.extensions.feedexport.GCSFeedStorage</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.extensions.feedexport.S3FeedStorage"></a><code><a href="scrapy.extensions.feedexport.S3FeedStorage.html" class="internal-link">scrapy.extensions.feedexport.S3FeedStorage</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><a name="scrapy.extensions.feedexport.FeedExporter"></a><code><a href="scrapy.extensions.feedexport.FeedExporter.html" class="internal-link">scrapy.extensions.feedexport.FeedExporter</a></code> - <span class="undocumented">No class docstring; 0/7 instance variable, 3/17 methods, 0/1 class method documented</span></li><li><a name="scrapy.extensions.feedexport.FileFeedStorage"></a><code><a href="scrapy.extensions.feedexport.FileFeedStorage.html" class="internal-link">scrapy.extensions.feedexport.FileFeedStorage</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.extensions.feedexport.ItemFilter"></a><code><a href="scrapy.extensions.feedexport.ItemFilter.html" class="internal-link">scrapy.extensions.feedexport.ItemFilter</a></code> - This will be used by FeedExporter to decide if an item should be allowed to be exported to a particular feed.</li><li><a name="scrapy.extensions.feedexport.StdoutFeedStorage"></a><code><a href="scrapy.extensions.feedexport.StdoutFeedStorage.html" class="internal-link">scrapy.extensions.feedexport.StdoutFeedStorage</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.extensions.httpcache.DbmCacheStorage"></a><code><a href="scrapy.extensions.httpcache.DbmCacheStorage.html" class="internal-link">scrapy.extensions.httpcache.DbmCacheStorage</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.extensions.httpcache.DummyPolicy"></a><code><a href="scrapy.extensions.httpcache.DummyPolicy.html" class="internal-link">scrapy.extensions.httpcache.DummyPolicy</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.extensions.httpcache.FilesystemCacheStorage"></a><code><a href="scrapy.extensions.httpcache.FilesystemCacheStorage.html" class="internal-link">scrapy.extensions.httpcache.FilesystemCacheStorage</a></code> - <span class="undocumented">No class docstring; 0/5 instance variable, 2/7 methods documented</span></li><li><a name="scrapy.extensions.httpcache.RFC2616Policy"></a><code><a href="scrapy.extensions.httpcache.RFC2616Policy.html" class="internal-link">scrapy.extensions.httpcache.RFC2616Policy</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.extensions.logstats.LogStats"></a><code><a href="scrapy.extensions.logstats.LogStats.html" class="internal-link">scrapy.extensions.logstats.LogStats</a></code> - Log basic scraping stats periodically</li><li><a name="scrapy.extensions.memdebug.MemoryDebugger"></a><code><a href="scrapy.extensions.memdebug.MemoryDebugger.html" class="internal-link">scrapy.extensions.memdebug.MemoryDebugger</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.extensions.memusage.MemoryUsage"></a><code><a href="scrapy.extensions.memusage.MemoryUsage.html" class="internal-link">scrapy.extensions.memusage.MemoryUsage</a></code> - <span class="undocumented">No class docstring; 0/9 instance variable, 1/8 method, 0/1 class method documented</span></li><li><a name="scrapy.extensions.postprocessing.Bz2Plugin"></a><code><a href="scrapy.extensions.postprocessing.Bz2Plugin.html" class="internal-link">scrapy.extensions.postprocessing.Bz2Plugin</a></code> - Compresses received data using `bz2 &lt;https://en.wikipedia.org/wiki/Bzip2&gt;`_.</li><li><a name="scrapy.extensions.postprocessing.GzipPlugin"></a><code><a href="scrapy.extensions.postprocessing.GzipPlugin.html" class="internal-link">scrapy.extensions.postprocessing.GzipPlugin</a></code> - Compresses received data using `gzip &lt;https://en.wikipedia.org/wiki/Gzip&gt;`_.</li><li><a name="scrapy.extensions.postprocessing.LZMAPlugin"></a><code><a href="scrapy.extensions.postprocessing.LZMAPlugin.html" class="internal-link">scrapy.extensions.postprocessing.LZMAPlugin</a></code> - Compresses received data using `lzma &lt;https://en.wikipedia.org/wiki/Lempel–Ziv–Markov_chain_algorithm&gt;`_.</li><li><a name="scrapy.extensions.spiderstate.SpiderState"></a><code><a href="scrapy.extensions.spiderstate.SpiderState.html" class="internal-link">scrapy.extensions.spiderstate.SpiderState</a></code> - Store and load spider state during a scraping job</li><li><a name="scrapy.extensions.statsmailer.StatsMailer"></a><code><a href="scrapy.extensions.statsmailer.StatsMailer.html" class="internal-link">scrapy.extensions.statsmailer.StatsMailer</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.extensions.throttle.AutoThrottle"></a><code><a href="scrapy.extensions.throttle.AutoThrottle.html" class="internal-link">scrapy.extensions.throttle.AutoThrottle</a></code> - <span class="undocumented">No class docstring; 0/5 instance variable, 1/8 method, 0/1 class method documented</span></li><li class="private"><a name="scrapy.http.cookies._DummyLock"></a><code><a href="scrapy.http.cookies._DummyLock.html" class="internal-link">scrapy.http.cookies._DummyLock</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.http.cookies.CookieJar"></a><code><a href="scrapy.http.cookies.CookieJar.html" class="internal-link">scrapy.http.cookies.CookieJar</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.http.cookies.WrappedRequest"></a><code><a href="scrapy.http.cookies.WrappedRequest.html" class="internal-link">scrapy.http.cookies.WrappedRequest</a></code> - Wraps a scrapy Request class with methods defined by urllib2.Request class to interact with CookieJar class</li><li><a name="scrapy.http.cookies.WrappedResponse"></a><code><a href="scrapy.http.cookies.WrappedResponse.html" class="internal-link">scrapy.http.cookies.WrappedResponse</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.link.Link"></a><code><a href="scrapy.link.Link.html" class="internal-link">scrapy.link.Link</a></code> - Link objects represent an extracted link by the LinkExtractor.</li><li><a name="scrapy.linkextractors.FilteringLinkExtractor"></a><code><a href="scrapy.linkextractors.FilteringLinkExtractor.html" class="internal-link">scrapy.linkextractors.FilteringLinkExtractor</a></code> - <span class="undocumented">Undocumented</span><ul><li><a name="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"></a><code><a href="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.html" class="internal-link">scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor</a></code> - <span class="undocumented">No class docstring; 1/2 method documented</span></li></ul></li><li><a name="scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor"></a><code><a href="scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor.html" class="internal-link">scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor</a></code> - <span class="undocumented">No class docstring; 0/6 instance variable, 1/6 method documented</span></li><li><a name="scrapy.logformatter.LogFormatter"></a><code><a href="scrapy.logformatter.LogFormatter.html" class="internal-link">scrapy.logformatter.LogFormatter</a></code> - Class for generating log messages for different actions.</li><li><a name="scrapy.mail.MailSender"></a><code><a href="scrapy.mail.MailSender.html" class="internal-link">scrapy.mail.MailSender</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.middleware.MiddlewareManager"></a><code><a href="scrapy.middleware.MiddlewareManager.html" class="internal-link">scrapy.middleware.MiddlewareManager</a></code> - Base class for implementing middleware managers<ul><li><a name="scrapy.core.downloader.middleware.DownloaderMiddlewareManager"></a><code><a href="scrapy.core.downloader.middleware.DownloaderMiddlewareManager.html" class="internal-link">scrapy.core.downloader.middleware.DownloaderMiddlewareManager</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.core.spidermw.SpiderMiddlewareManager"></a><code><a href="scrapy.core.spidermw.SpiderMiddlewareManager.html" class="internal-link">scrapy.core.spidermw.SpiderMiddlewareManager</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.extension.ExtensionManager"></a><code><a href="scrapy.extension.ExtensionManager.html" class="internal-link">scrapy.extension.ExtensionManager</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.pipelines.ItemPipelineManager"></a><code><a href="scrapy.pipelines.ItemPipelineManager.html" class="internal-link">scrapy.pipelines.ItemPipelineManager</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><a name="scrapy.pipelines.files.FSFilesStore"></a><code><a href="scrapy.pipelines.files.FSFilesStore.html" class="internal-link">scrapy.pipelines.files.FSFilesStore</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.pipelines.files.FTPFilesStore"></a><code><a href="scrapy.pipelines.files.FTPFilesStore.html" class="internal-link">scrapy.pipelines.files.FTPFilesStore</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.pipelines.files.GCSFilesStore"></a><code><a href="scrapy.pipelines.files.GCSFilesStore.html" class="internal-link">scrapy.pipelines.files.GCSFilesStore</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.pipelines.files.S3FilesStore"></a><code><a href="scrapy.pipelines.files.S3FilesStore.html" class="internal-link">scrapy.pipelines.files.S3FilesStore</a></code> - <span class="undocumented">No class docstring; 0/3 instance variable, 0/9 constant, 2/5 methods documented</span></li><li><a name="scrapy.pipelines.media.MediaPipeline"></a><code><a href="scrapy.pipelines.media.MediaPipeline.html" class="internal-link">scrapy.pipelines.media.MediaPipeline</a></code> - <span class="undocumented">No class docstring; 0/5 instance variable, 0/1 constant, 9/18 methods, 0/1 class method, 0/1 class documented</span><ul><li><a name="scrapy.pipelines.files.FilesPipeline"></a><code><a href="scrapy.pipelines.files.FilesPipeline.html" class="internal-link">scrapy.pipelines.files.FilesPipeline</a></code> - Abstract pipeline that implement the file downloading<ul><li><a name="scrapy.pipelines.images.ImagesPipeline"></a><code><a href="scrapy.pipelines.images.ImagesPipeline.html" class="internal-link">scrapy.pipelines.images.ImagesPipeline</a></code> - Abstract pipeline that implement the image thumbnail generation logic</li></ul></li></ul></li><li><a name="scrapy.pipelines.media.MediaPipeline.SpiderInfo"></a><code><a href="scrapy.pipelines.media.MediaPipeline.SpiderInfo.html" class="internal-link">scrapy.pipelines.media.MediaPipeline.SpiderInfo</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.pqueues.DownloaderAwarePriorityQueue"></a><code><a href="scrapy.pqueues.DownloaderAwarePriorityQueue.html" class="internal-link">scrapy.pqueues.DownloaderAwarePriorityQueue</a></code> - PriorityQueue which takes Downloader activity into account: domains (slots) with the least amount of active downloads are dequeued first.</li><li><a name="scrapy.pqueues.DownloaderInterface"></a><code><a href="scrapy.pqueues.DownloaderInterface.html" class="internal-link">scrapy.pqueues.DownloaderInterface</a></code> - <span class="undocumented">No class docstring; 0/1 instance variable, 1/4 method documented</span></li><li><a name="scrapy.pqueues.ScrapyPriorityQueue"></a><code><a href="scrapy.pqueues.ScrapyPriorityQueue.html" class="internal-link">scrapy.pqueues.ScrapyPriorityQueue</a></code> - A priority queue implemented using multiple internal queues (typically, FIFO queues). It uses one internal queue for each priority value. The internal queue must implement the following methods:</li><li class="private"><a name="scrapy.resolver._CachingResolutionReceiver"></a><code><a href="scrapy.resolver._CachingResolutionReceiver.html" class="internal-link">scrapy.resolver._CachingResolutionReceiver</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.resolver.CachingHostnameResolver"></a><code><a href="scrapy.resolver.CachingHostnameResolver.html" class="internal-link">scrapy.resolver.CachingHostnameResolver</a></code> - Experimental caching resolver. Resolves IPv4 and IPv6 addresses, does not support setting a timeout value for DNS requests.</li><li><a name="scrapy.resolver.HostResolution"></a><code><a href="scrapy.resolver.HostResolution.html" class="internal-link">scrapy.resolver.HostResolution</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.responsetypes.ResponseTypes"></a><code><a href="scrapy.responsetypes.ResponseTypes.html" class="internal-link">scrapy.responsetypes.ResponseTypes</a></code> - <span class="undocumented">No class docstring; 0/2 instance variable, 0/1 constant, 6/8 methods documented</span></li><li><a name="scrapy.robotstxt.RobotParser"></a><code><a href="scrapy.robotstxt.RobotParser.html" class="internal-link">scrapy.robotstxt.RobotParser</a></code> - <span class="undocumented">No class docstring; 1/1 method, 1/1 class method documented</span><ul><li><a name="scrapy.robotstxt.ProtegoRobotParser"></a><code><a href="scrapy.robotstxt.ProtegoRobotParser.html" class="internal-link">scrapy.robotstxt.ProtegoRobotParser</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.robotstxt.PythonRobotParser"></a><code><a href="scrapy.robotstxt.PythonRobotParser.html" class="internal-link">scrapy.robotstxt.PythonRobotParser</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.robotstxt.ReppyRobotParser"></a><code><a href="scrapy.robotstxt.ReppyRobotParser.html" class="internal-link">scrapy.robotstxt.ReppyRobotParser</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.robotstxt.RerpRobotParser"></a><code><a href="scrapy.robotstxt.RerpRobotParser.html" class="internal-link">scrapy.robotstxt.RerpRobotParser</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><a name="scrapy.settings.SettingsAttribute"></a><code><a href="scrapy.settings.SettingsAttribute.html" class="internal-link">scrapy.settings.SettingsAttribute</a></code> - Class for storing data related to settings attributes.</li><li><a name="scrapy.shell.Shell"></a><code><a href="scrapy.shell.Shell.html" class="internal-link">scrapy.shell.Shell</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.signalmanager.SignalManager"></a><code><a href="scrapy.signalmanager.SignalManager.html" class="internal-link">scrapy.signalmanager.SignalManager</a></code> - <span class="undocumented">No class docstring; 0/1 instance variable, 5/6 methods documented</span></li><li><a name="scrapy.spiderloader.SpiderLoader"></a><code><a href="scrapy.spiderloader.SpiderLoader.html" class="internal-link">scrapy.spiderloader.SpiderLoader</a></code> - SpiderLoader is a class which locates and loads spiders in a Scrapy project.</li><li><a name="scrapy.spidermiddlewares.depth.DepthMiddleware"></a><code><a href="scrapy.spidermiddlewares.depth.DepthMiddleware.html" class="internal-link">scrapy.spidermiddlewares.depth.DepthMiddleware</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.spidermiddlewares.httperror.HttpErrorMiddleware"></a><code><a href="scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.html" class="internal-link">scrapy.spidermiddlewares.httperror.HttpErrorMiddleware</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.spidermiddlewares.offsite.OffsiteMiddleware"></a><code><a href="scrapy.spidermiddlewares.offsite.OffsiteMiddleware.html" class="internal-link">scrapy.spidermiddlewares.offsite.OffsiteMiddleware</a></code> - <span class="undocumented">No class docstring; 0/3 instance variable, 1/7 method, 0/1 class method documented</span></li><li><a name="scrapy.spidermiddlewares.referer.RefererMiddleware"></a><code><a href="scrapy.spidermiddlewares.referer.RefererMiddleware.html" class="internal-link">scrapy.spidermiddlewares.referer.RefererMiddleware</a></code> - <span class="undocumented">No class docstring; 0/1 instance variable, 1/6 method, 0/1 class method documented</span></li><li><a name="scrapy.spidermiddlewares.referer.ReferrerPolicy"></a><code><a href="scrapy.spidermiddlewares.referer.ReferrerPolicy.html" class="internal-link">scrapy.spidermiddlewares.referer.ReferrerPolicy</a></code> - <span class="undocumented">No class docstring; 0/1 class variable, 2/7 methods documented</span><ul><li><a name="scrapy.spidermiddlewares.referer.NoReferrerPolicy"></a><code><a href="scrapy.spidermiddlewares.referer.NoReferrerPolicy.html" class="internal-link">scrapy.spidermiddlewares.referer.NoReferrerPolicy</a></code> - https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer</li><li><a name="scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy"></a><code><a href="scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy.html" class="internal-link">scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy</a></code> - https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade<ul><li><a name="scrapy.spidermiddlewares.referer.DefaultReferrerPolicy"></a><code><a href="scrapy.spidermiddlewares.referer.DefaultReferrerPolicy.html" class="internal-link">scrapy.spidermiddlewares.referer.DefaultReferrerPolicy</a></code> - A variant of "no-referrer-when-downgrade", with the addition that "Referer" is not sent if the parent request was using ``file://`` or ``s3://`` scheme.</li></ul></li><li><a name="scrapy.spidermiddlewares.referer.OriginPolicy"></a><code><a href="scrapy.spidermiddlewares.referer.OriginPolicy.html" class="internal-link">scrapy.spidermiddlewares.referer.OriginPolicy</a></code> - https://www.w3.org/TR/referrer-policy/#referrer-policy-origin</li><li><a name="scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy"></a><code><a href="scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy.html" class="internal-link">scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy</a></code> - https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin</li><li><a name="scrapy.spidermiddlewares.referer.SameOriginPolicy"></a><code><a href="scrapy.spidermiddlewares.referer.SameOriginPolicy.html" class="internal-link">scrapy.spidermiddlewares.referer.SameOriginPolicy</a></code> - https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin</li><li><a name="scrapy.spidermiddlewares.referer.StrictOriginPolicy"></a><code><a href="scrapy.spidermiddlewares.referer.StrictOriginPolicy.html" class="internal-link">scrapy.spidermiddlewares.referer.StrictOriginPolicy</a></code> - https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin</li><li><a name="scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy"></a><code><a href="scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy.html" class="internal-link">scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy</a></code> - https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin</li><li><a name="scrapy.spidermiddlewares.referer.UnsafeUrlPolicy"></a><code><a href="scrapy.spidermiddlewares.referer.UnsafeUrlPolicy.html" class="internal-link">scrapy.spidermiddlewares.referer.UnsafeUrlPolicy</a></code> - https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url</li></ul></li><li><a name="scrapy.spidermiddlewares.urllength.UrlLengthMiddleware"></a><code><a href="scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.html" class="internal-link">scrapy.spidermiddlewares.urllength.UrlLengthMiddleware</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.spiders.crawl.Rule"></a><code><a href="scrapy.spiders.crawl.Rule.html" class="internal-link">scrapy.spiders.crawl.Rule</a></code> - <span class="undocumented">Undocumented</span></li><li><code>scrapy.spiders.Spider</code><ul><li><a name="scrapy.spiders.init.InitSpider"></a><code><a href="scrapy.spiders.init.InitSpider.html" class="internal-link">scrapy.spiders.init.InitSpider</a></code> - Base Spider with initialization facilities</li></ul></li><li><a name="scrapy.statscollectors.StatsCollector"></a><code><a href="scrapy.statscollectors.StatsCollector.html" class="internal-link">scrapy.statscollectors.StatsCollector</a></code> - <span class="undocumented">Undocumented</span><ul><li><a name="scrapy.statscollectors.DummyStatsCollector"></a><code><a href="scrapy.statscollectors.DummyStatsCollector.html" class="internal-link">scrapy.statscollectors.DummyStatsCollector</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.statscollectors.MemoryStatsCollector"></a><code><a href="scrapy.statscollectors.MemoryStatsCollector.html" class="internal-link">scrapy.statscollectors.MemoryStatsCollector</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><a name="scrapy.utils.datatypes.SequenceExclude"></a><code><a href="scrapy.utils.datatypes.SequenceExclude.html" class="internal-link">scrapy.utils.datatypes.SequenceExclude</a></code> - Object to test if an item is NOT within some sequence.</li><li class="private"><a name="scrapy.utils.iterators._StreamReader"></a><code><a href="scrapy.utils.iterators._StreamReader.html" class="internal-link">scrapy.utils.iterators._StreamReader</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.utils.log.StreamLogger"></a><code><a href="scrapy.utils.log.StreamLogger.html" class="internal-link">scrapy.utils.log.StreamLogger</a></code> - Fake file-like stream object that redirects writes to a logger instance</li><li><a name="scrapy.utils.python.WeakKeyCache"></a><code><a href="scrapy.utils.python.WeakKeyCache.html" class="internal-link">scrapy.utils.python.WeakKeyCache</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.utils.reactor.CallLaterOnce"></a><code><a href="scrapy.utils.reactor.CallLaterOnce.html" class="internal-link">scrapy.utils.reactor.CallLaterOnce</a></code> - Schedule a function to be called in the next reactor loop, but only if it hasn't been already scheduled since the last time it ran.</li><li><a name="scrapy.utils.request.RequestFingerprinter"></a><code><a href="scrapy.utils.request.RequestFingerprinter.html" class="internal-link">scrapy.utils.request.RequestFingerprinter</a></code> - Default fingerprinter.</li><li><a name="scrapy.utils.sitemap.Sitemap"></a><code><a href="scrapy.utils.sitemap.Sitemap.html" class="internal-link">scrapy.utils.sitemap.Sitemap</a></code> - Class to parse Sitemap (type=urlset) and Sitemap Index (type=sitemapindex) files</li><li><a name="scrapy.utils.testproc.ProcessTest"></a><code><a href="scrapy.utils.testproc.ProcessTest.html" class="internal-link">scrapy.utils.testproc.ProcessTest</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.utils.testsite.SiteTest"></a><code><a href="scrapy.utils.testsite.SiteTest.html" class="internal-link">scrapy.utils.testsite.SiteTest</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.utils.trackref.object_ref"></a><code><a href="scrapy.utils.trackref.object_ref.html" class="internal-link">scrapy.utils.trackref.object_ref</a></code> - Inherit from this class to a keep a record of live instances<ul><li><a name="scrapy.http.response.Response"></a><code><a href="scrapy.http.response.Response.html" class="internal-link">scrapy.http.response.Response</a></code> - An object that represents an HTTP response, which is usually downloaded (by the Downloader) and fed to the Spiders for processing.<ul><li><a name="scrapy.http.response.text.TextResponse"></a><code><a href="scrapy.http.response.text.TextResponse.html" class="internal-link">scrapy.http.response.text.TextResponse</a></code> - <span class="undocumented">No class docstring; 1/3 property, 0/7 instance variable, 0/1 class variable, 0/1 constant, 4/15 methods documented</span><ul><li><a name="scrapy.http.response.html.HtmlResponse"></a><code><a href="scrapy.http.response.html.HtmlResponse.html" class="internal-link">scrapy.http.response.html.HtmlResponse</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.http.response.xml.XmlResponse"></a><code><a href="scrapy.http.response.xml.XmlResponse.html" class="internal-link">scrapy.http.response.xml.XmlResponse</a></code> - <span class="undocumented">Undocumented</span></li></ul></li></ul></li><li><code><a href="scrapy.Item.html" class="internal-link">scrapy.Item</a></code> - Base class for scraped items.</li><li><a name="scrapy.Request"></a><code><a href="scrapy.Request.html" class="internal-link">scrapy.Request</a></code> - Represents an HTTP request, which is usually generated in a Spider and executed by the Downloader, thus generating a :class:`Response`.<ul><li><a name="scrapy.FormRequest"></a><code><a href="scrapy.FormRequest.html" class="internal-link">scrapy.FormRequest</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.http.request.json_request.JsonRequest"></a><code><a href="scrapy.http.request.json_request.JsonRequest.html" class="internal-link">scrapy.http.request.json_request.JsonRequest</a></code> - <span class="undocumented">No class docstring; 0/1 property, 0/1 instance variable, 0/1 class variable, 1/3 method documented</span></li><li><a name="scrapy.http.request.rpc.XmlRpcRequest"></a><code><a href="scrapy.http.request.rpc.XmlRpcRequest.html" class="internal-link">scrapy.http.request.rpc.XmlRpcRequest</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><code><a href="scrapy.Selector.html" class="internal-link">scrapy.Selector</a></code> - An instance of :class:`Selector` is a wrapper over response to select certain parts of its content.</li><li><code><a href="scrapy.selector.unified.SelectorList.html" class="internal-link">scrapy.selector.unified.SelectorList</a></code> - The :class:`SelectorList` class is a subclass of the builtin ``list`` class, which provides a few additional methods.</li><li><a name="scrapy.Spider"></a><code><a href="scrapy.Spider.html" class="internal-link">scrapy.Spider</a></code> - Base class for scrapy spiders. All spiders must inherit from this class.<ul><li class="private"><a name="scrapy.commands.bench._BenchSpider"></a><code><a href="scrapy.commands.bench._BenchSpider.html" class="internal-link">scrapy.commands.bench._BenchSpider</a></code> - A spider that follows all links</li><li><a name="scrapy.spiders.crawl.CrawlSpider"></a><code><a href="scrapy.spiders.crawl.CrawlSpider.html" class="internal-link">scrapy.spiders.crawl.CrawlSpider</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.spiders.feed.CSVFeedSpider"></a><code><a href="scrapy.spiders.feed.CSVFeedSpider.html" class="internal-link">scrapy.spiders.feed.CSVFeedSpider</a></code> - Spider for parsing CSV feeds. It receives a CSV file in a response; iterates through each of its rows, and calls parse_row with a dict containing each field's data.</li><li><a name="scrapy.spiders.feed.XMLFeedSpider"></a><code><a href="scrapy.spiders.feed.XMLFeedSpider.html" class="internal-link">scrapy.spiders.feed.XMLFeedSpider</a></code> - This class intends to be the base class for spiders that scrape from XML feeds.</li><li><a name="scrapy.spiders.sitemap.SitemapSpider"></a><code><a href="scrapy.spiders.sitemap.SitemapSpider.html" class="internal-link">scrapy.spiders.sitemap.SitemapSpider</a></code> - <span class="undocumented">No class docstring; 0/2 instance variable, 0/4 class variable, 2/5 methods documented</span></li><li><a name="scrapy.utils.spider.DefaultSpider"></a><code><a href="scrapy.utils.spider.DefaultSpider.html" class="internal-link">scrapy.utils.spider.DefaultSpider</a></code> - <span class="undocumented">Undocumented</span></li></ul></li></ul></li><li><code>twisted.internet._sslverify.ClientTLSOptions</code><ul><li><a name="scrapy.core.downloader.tls.ScrapyClientTLSOptions"></a><code><a href="scrapy.core.downloader.tls.ScrapyClientTLSOptions.html" class="internal-link">scrapy.core.downloader.tls.ScrapyClientTLSOptions</a></code> - SSL Client connection creator ignoring certificate verification errors (for genuinely invalid certificates or bugs in verification code).</li></ul></li><li><code>twisted.internet.base.ThreadedResolver</code><ul><li><a name="scrapy.resolver.CachingThreadedResolver"></a><code><a href="scrapy.resolver.CachingThreadedResolver.html" class="internal-link">scrapy.resolver.CachingThreadedResolver</a></code> - Default caching resolver. IPv4 only, supports setting a timeout value for DNS requests.</li></ul></li><li><code>twisted.internet.endpoints.TCP4ClientEndpoint</code><ul><li><a name="scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint"></a><code><a href="scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.html" class="internal-link">scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint</a></code> - An endpoint that tunnels through proxies to allow HTTPS downloads. To accomplish that, this endpoint sends an HTTP CONNECT to the proxy. The HTTP CONNECT is always sent when using this endpoint, I think this could be improved as the CONNECT will be redundant if the connection associated with this endpoint comes from the pool and a CONNECT has already been issued for it.</li></ul></li><li><code>twisted.internet.error.ConnectionClosed</code><ul><li><a name="scrapy.core.http2.stream.InactiveStreamClosed"></a><code><a href="scrapy.core.http2.stream.InactiveStreamClosed.html" class="internal-link">scrapy.core.http2.stream.InactiveStreamClosed</a></code> - Connection was closed without sending request headers of the stream. This happens when a stream is waiting for other streams to close and connection is lost.</li></ul></li><li><code>twisted.internet.protocol.ClientFactory</code><ul><li><a name="scrapy.core.downloader.webclient.ScrapyHTTPClientFactory"></a><code><a href="scrapy.core.downloader.webclient.ScrapyHTTPClientFactory.html" class="internal-link">scrapy.core.downloader.webclient.ScrapyHTTPClientFactory</a></code> - <span class="undocumented">No class docstring; 0/20 instance variable, 0/3 class variable, 2/11 methods documented</span></li></ul></li><li><code>twisted.internet.protocol.Factory</code><ul><li><a name="scrapy.core.http2.protocol.H2ClientFactory"></a><code><a href="scrapy.core.http2.protocol.H2ClientFactory.html" class="internal-link">scrapy.core.http2.protocol.H2ClientFactory</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><code>twisted.internet.protocol.ProcessProtocol</code><ul><li><a name="scrapy.utils.testproc.TestProcessProtocol"></a><code><a href="scrapy.utils.testproc.TestProcessProtocol.html" class="internal-link">scrapy.utils.testproc.TestProcessProtocol</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><code>twisted.internet.protocol.Protocol</code><ul><li><a name="scrapy.core.downloader.handlers.ftp.ReceivedDataProtocol"></a><code><a href="scrapy.core.downloader.handlers.ftp.ReceivedDataProtocol.html" class="internal-link">scrapy.core.downloader.handlers.ftp.ReceivedDataProtocol</a></code> - <span class="undocumented">Undocumented</span></li><li class="private"><a name="scrapy.core.downloader.handlers.http11._ResponseReader"></a><code><a href="scrapy.core.downloader.handlers.http11._ResponseReader.html" class="internal-link">scrapy.core.downloader.handlers.http11._ResponseReader</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.core.http2.protocol.H2ClientProtocol"></a><code><a href="scrapy.core.http2.protocol.H2ClientProtocol.html" class="internal-link">scrapy.core.http2.protocol.H2ClientProtocol</a></code> - <span class="undocumented">No class docstring; 2/2 properties, 0/7 instance variable, 0/1 constant, 12/21 methods documented</span></li></ul></li><li><code>twisted.internet.protocol.ServerFactory</code><ul><li><a name="scrapy.extensions.telnet.TelnetConsole"></a><code><a href="scrapy.extensions.telnet.TelnetConsole.html" class="internal-link">scrapy.extensions.telnet.TelnetConsole</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><code>twisted.protocols.policies.TimeoutMixin</code><ul><li><code><a href="scrapy.core.http2.protocol.H2ClientProtocol.html" class="internal-link">scrapy.core.http2.protocol.H2ClientProtocol</a></code> - <span class="undocumented">No class docstring; 2/2 properties, 0/7 instance variable, 0/1 constant, 12/21 methods documented</span></li></ul></li><li><code>twisted.web.client.Agent</code><ul><li><a name="scrapy.core.downloader.handlers.http11.ScrapyProxyAgent"></a><code><a href="scrapy.core.downloader.handlers.http11.ScrapyProxyAgent.html" class="internal-link">scrapy.core.downloader.handlers.http11.ScrapyProxyAgent</a></code> - <span class="undocumented">No class docstring; 0/1 instance variable, 1/2 method documented</span></li><li><a name="scrapy.core.downloader.handlers.http11.TunnelingAgent"></a><code><a href="scrapy.core.downloader.handlers.http11.TunnelingAgent.html" class="internal-link">scrapy.core.downloader.handlers.http11.TunnelingAgent</a></code> - An agent that uses a L{TunnelingTCP4ClientEndpoint} to make HTTPS downloads. It may look strange that we have chosen to subclass Agent and not ProxyAgent but consider that after the tunnel is opened the proxy is transparent to the client; thus the agent should behave like there is no proxy involved.</li></ul></li><li><code>twisted.web.client.BrowserLikePolicyForHTTPS</code><ul><li><a name="scrapy.core.downloader.contextfactory.ScrapyClientContextFactory"></a><code><a href="scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.html" class="internal-link">scrapy.core.downloader.contextfactory.ScrapyClientContextFactory</a></code> - Non-peer-certificate verifying HTTPS context factory<ul><li><a name="scrapy.core.downloader.contextfactory.BrowserLikeContextFactory"></a><code><a href="scrapy.core.downloader.contextfactory.BrowserLikeContextFactory.html" class="internal-link">scrapy.core.downloader.contextfactory.BrowserLikeContextFactory</a></code> - Twisted-recommended context factory for web clients.</li></ul></li></ul></li><li><code>twisted.web.http.HTTPClient</code><ul><li><a name="scrapy.core.downloader.webclient.ScrapyHTTPPageGetter"></a><code><a href="scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.html" class="internal-link">scrapy.core.downloader.webclient.ScrapyHTTPPageGetter</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><code>twisted.web.resource.Resource</code><ul><li><a name="scrapy.utils.benchserver.Root"></a><code><a href="scrapy.utils.benchserver.Root.html" class="internal-link">scrapy.utils.benchserver.Root</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><code>twisted.web.util.Redirect</code><ul><li><a name="scrapy.utils.testsite.NoMetaRefreshRedirect"></a><code><a href="scrapy.utils.testsite.NoMetaRefreshRedirect.html" class="internal-link">scrapy.utils.testsite.NoMetaRefreshRedirect</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><code>type</code><ul><li><a name="scrapy.core.scheduler.BaseSchedulerMeta"></a><code><a href="scrapy.core.scheduler.BaseSchedulerMeta.html" class="internal-link">scrapy.core.scheduler.BaseSchedulerMeta</a></code> - Metaclass to check scheduler classes against the necessary interface</li></ul></li><li class="private"><code>TypeError</code><ul><li class="private"><a name="scrapy.exceptions._InvalidOutput"></a><code><a href="scrapy.exceptions._InvalidOutput.html" class="internal-link">scrapy.exceptions._InvalidOutput</a></code> - Indicates an invalid value has been returned by a middleware's processing method. Internal and undocumented, it should not be raised or caught by user code.</li></ul></li><li><code>typing.AsyncIterable</code><ul><li><a name="scrapy.utils.python.MutableAsyncChain"></a><code><a href="scrapy.utils.python.MutableAsyncChain.html" class="internal-link">scrapy.utils.python.MutableAsyncChain</a></code> - Similar to MutableChain but for async iterables</li></ul></li><li><code>typing.Iterable</code><ul><li><a name="scrapy.utils.python.MutableChain"></a><code><a href="scrapy.utils.python.MutableChain.html" class="internal-link">scrapy.utils.python.MutableChain</a></code> - Thin wrapper around itertools.chain, allowing to add iterables "in-place"</li></ul></li><li class="private"><code>typing.Iterator</code><ul><li class="private"><a name="scrapy.utils.defer._AsyncCooperatorAdapter"></a><code><a href="scrapy.utils.defer._AsyncCooperatorAdapter.html" class="internal-link">scrapy.utils.defer._AsyncCooperatorAdapter</a></code> - A class that wraps an async iterable into a normal iterator suitable for using in Cooperator.coiterate(). As it's only needed for parallel_async(), it calls the callable directly in the callback, instead of providing a more generic interface.</li></ul></li><li><code>unittest.TextTestResult</code><ul><li><a name="scrapy.commands.check.TextTestResult"></a><code><a href="scrapy.commands.check.TextTestResult.html" class="internal-link">scrapy.commands.check.TextTestResult</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li class="private"><code>ValueError</code><ul><li class="private"><a name="scrapy.http.response.text._InvalidSelector"></a><code><a href="scrapy.http.response.text._InvalidSelector.html" class="internal-link">scrapy.http.response.text._InvalidSelector</a></code> - Raised when a URL cannot be obtained from a Selector</li></ul></li><li><code>Warning</code><ul><li><a name="scrapy.exceptions.ScrapyDeprecationWarning"></a><code><a href="scrapy.exceptions.ScrapyDeprecationWarning.html" class="internal-link">scrapy.exceptions.ScrapyDeprecationWarning</a></code> - Warning category for deprecated features, since the default DeprecationWarning is silenced on Python 2.7+</li><li><a name="scrapy.spidermiddlewares.offsite.PortWarning"></a><code><a href="scrapy.spidermiddlewares.offsite.PortWarning.html" class="internal-link">scrapy.spidermiddlewares.offsite.PortWarning</a></code> - <span class="undocumented">Undocumented</span></li><li><a name="scrapy.spidermiddlewares.offsite.URLWarning"></a><code><a href="scrapy.spidermiddlewares.offsite.URLWarning.html" class="internal-link">scrapy.spidermiddlewares.offsite.URLWarning</a></code> - <span class="undocumented">Undocumented</span></li></ul></li><li><code>weakref.WeakKeyDictionary</code><ul><li><a name="scrapy.utils.datatypes.LocalWeakReferencedCache"></a><code><a href="scrapy.utils.datatypes.LocalWeakReferencedCache.html" class="internal-link">scrapy.utils.datatypes.LocalWeakReferencedCache</a></code> - A weakref.WeakKeyDictionary implementation that uses LocalCache as its underlying data structure, making it ordered and capable of being size-limited.</li></ul></li><li><code>zope.interface.Interface</code><ul><li><a name="scrapy.extensions.feedexport.IFeedStorage"></a><code><a href="scrapy.extensions.feedexport.IFeedStorage.html" class="internal-link">scrapy.extensions.feedexport.IFeedStorage</a></code> - Interface that all Feed Storages must implement</li><li><a name="scrapy.interfaces.ISpiderLoader"></a><code><a href="scrapy.interfaces.ISpiderLoader.html" class="internal-link">scrapy.interfaces.ISpiderLoader</a></code> - <span class="undocumented">No interface docstring; 4/4 methods documented</span></li></ul></li></ul>

    </div>

    <footer class="navbar navbar-default">

  
  <div class="container">
    <a href="index.html">API Documentation</a> for scrapy,
  generated by <a href="https://github.com/twisted/pydoctor/">pydoctor</a>
    22.9.1 at 2022-10-18 01:26:11.
  </div>

  <!-- Search related scripts needs to be loaded at the end of HTML 
    parsing not to introduce overhead and display HTML data asap -->
  <script src="ajax.js" type="text/javascript"></script>
  <script src="searchlib.js" type="text/javascript"></script>
  <script src="search.js" type="text/javascript"></script>

</footer>

    <script src="pydoctor.js" type="text/javascript"></script>

  </body>
</html>
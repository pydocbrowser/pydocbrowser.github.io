<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
          "DTD/xhtml1-strict.dtd">
<html>
  
  
  <head>
    
    <title>Class Hierarchy</title>
    <meta name="generator" content="pydoctor 22.9.1.dev0"> 
        
    </meta>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1 maximum-scale=1" />
    <link rel="stylesheet" type="text/css" href="bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="apidocs.css" />
    <link rel="stylesheet" type="text/css" href="extra.css" />
</head>

  <body>

    <div id="banner">    
    <div>
        <a href="/">Home</a>
        &gt; kedro-0.18.5 <!-- This is a placeholder -->
        
        <!-- (<a href=""&gt;show all versions</a&gt;) -->
    </div>
</div>

    <nav class="navbar navbar-default mainnavbar">
      
  
  <div class="container-fluid">


    <div class="navbar-header">
      
      <div class="navlinks">
        <span class="navbar-brand">
          kedro <a href="index.html">API Documentation</a>
        </span>

        <a href="moduleIndex.html">
          Modules
        </a>

        <a href="classIndex.html">
          Classes
        </a>

        <a href="nameIndex.html">
          Names
        </a>

        <div id="search-box-container">
          <div class="input-group">
            <input id="search-box" type="search" name="search-query" placeholder="Search..." aria-label="Search" minlength="2" class="form-control" autocomplete="off" />
            
            <span class="input-group-btn">
              <a style="display: none;" class="btn btn-default" id="search-clear-button" title="Clear" onclick="clearSearch()"><img src="fonts/x-circle.svg" alt="Clear" /></a>
              <a class="btn btn-default" id="search-help-button" title="Help" onclick="toggleSearchHelpText()"><img src="fonts/info.svg" alt="Help" /></a>
            </span>
          </div>
        </div>

      </div>

      <div id="search-results-container" style="display: none;">
        
        <div id="search-buttons">
          <span class="label label-default" id="search-docstrings-button">
          <label class="checkbox-inline">
            <input type="checkbox" id="toggle-search-in-docstrings-checkbox" value="false" onclick="toggleSearchInDocstrings()">
              search in docstrings
            </input>
          </label>
          </span>
          
        </div>
        
        <noscript>
            <h1>Cannot search: JavaScript is not supported/enabled in your browser.</h1>
        </noscript>

        <div class="hint" id="search-help-box">
          <p class="rst-last">
      
            Search bar offers the following options:
            <ul>   
                <li>
                  <strong>Term presence.</strong> The below example searches for documents that 
                    must contain “foo”, might contain “bar” and must not contain “baz”: <code>+foo bar -baz</code>
                </li> 

                <li>
                  <strong>Wildcards.</strong> The below example searches for documents with words beginning with “foo”: <code>foo*</code>
                </li> 

                <li>
                  <strong>Search in specific fields.</strong> The following search matches all objects 
                  in "twisted.mail" that matches “search”: <code>+qname:twisted.mail.* +search</code>

                  <p>
                    Possible fields: 'name', 'qname' (fully qualified name), 'docstring', and 'kind'.
                    Last two fields are only applicable if "search in docstrings" is enabled.
                  </p>
                </li>

                <li>
                  <strong>Fuzzy matches.</strong> The following search matches all documents 
                  that have a word within 1 edit distance of “foo”: <code>foo~1</code>
                </li>
            </ul>

          </p>
        </div>

        <div id="search-status"> </div>

        <div class="warning" id="search-warn-box" style="display: none;">
          <p class="rst-last"><span id="search-warn"></span></p>
        </div>

        <table id="search-results">
          <!-- Filled dynamically by JS -->
        </table>
        
        <div style="margin-top: 8px;">
          <p>Results provided by <a href="https://lunrjs.com">Lunr.js</a></p>
        </div>
      </div>

    </div>


  </div>
</nav>

    <div class="container-fluid">

      <div class="page-header">
        <h1>Class Hierarchy</h1>
        <div id="showPrivate">
          <button class="btn btn-link" onclick="togglePrivate()">Toggle Private API</button>
        </div>
      </div>

      <ul id="summaryTree">
      <li><code>abc.ABC</code><ul><li><a name="kedro.io.AbstractDataSet"></a><code><a href="kedro.io.AbstractDataSet.html" class="internal-link">kedro.io.AbstractDataSet</a></code> - <tt class="rst-docutils literal">AbstractDataSet</tt> is the base class for all data set implementations. All data set implementations should extend this abstract class and implement the methods marked as abstract. If a specific dataset implementation cannot be used in conjunction with the ...<ul><li><a name="kedro.extras.datasets.api.APIDataSet"></a><code><a href="kedro.extras.datasets.api.APIDataSet.html" class="internal-link">kedro.extras.datasets.api.APIDataSet</a></code> - <tt class="rst-docutils literal">APIDataSet</tt> loads the data from HTTP(S) APIs. It uses the python requests library: <a class="rst-reference external" href="https://requests.readthedocs.io/en/latest/" target="_top">https://requests.readthedocs.io/en/latest/</a></li><li><a name="kedro.extras.datasets.biosequence.BioSequenceDataSet"></a><code><a href="kedro.extras.datasets.biosequence.BioSequenceDataSet.html" class="internal-link">kedro.extras.datasets.biosequence.BioSequenceDataSet</a></code> - <tt class="rst-docutils literal">BioSequenceDataSet</tt> loads and saves data to a sequence file.</li><li><a name="kedro.extras.datasets.dask.ParquetDataSet"></a><code><a href="kedro.extras.datasets.dask.ParquetDataSet.html" class="internal-link">kedro.extras.datasets.dask.ParquetDataSet</a></code> - <tt class="rst-docutils literal">ParquetDataSet</tt> loads and saves data to parquet file(s). It uses Dask remote data services to handle the corresponding load and save operations: <a class="rst-reference external" href="https://docs.dask.org/en/latest/how-to/connect-to-remote-data.html" target="_top">https://docs.dask.org/en/latest/how-to/connect-to-remote-data.html</a>...</li><li><a name="kedro.extras.datasets.pandas.GBQQueryDataSet"></a><code><a href="kedro.extras.datasets.pandas.GBQQueryDataSet.html" class="internal-link">kedro.extras.datasets.pandas.GBQQueryDataSet</a></code> - <tt class="rst-docutils literal">GBQQueryDataSet</tt> loads data from a provided SQL query from Google BigQuery. It uses <tt class="rst-docutils literal">pandas.read_gbq</tt> which itself uses <tt class="rst-docutils literal"><span class="pre">pandas-gbq</span></tt> internally to read from BigQuery table. Therefore it supports all allowed pandas options on ...</li><li><a name="kedro.extras.datasets.pandas.GBQTableDataSet"></a><code><a href="kedro.extras.datasets.pandas.GBQTableDataSet.html" class="internal-link">kedro.extras.datasets.pandas.GBQTableDataSet</a></code> - <tt class="rst-docutils literal">GBQTableDataSet</tt> loads and saves data from/to Google BigQuery. It uses pandas-gbq to read and write from/to BigQuery table.</li><li><a name="kedro.extras.datasets.pandas.sql_dataset.SQLQueryDataSet"></a><code><a href="kedro.extras.datasets.pandas.sql_dataset.SQLQueryDataSet.html" class="internal-link">kedro.extras.datasets.pandas.sql_dataset.SQLQueryDataSet</a></code> - <tt class="rst-docutils literal">SQLQueryDataSet</tt> loads data from a provided SQL query. It uses <tt class="rst-docutils literal">pandas.DataFrame</tt> internally, so it supports all allowed pandas options on <tt class="rst-docutils literal">read_sql_query</tt>. Since Pandas uses SQLAlchemy behind the scenes, when instantiating ...</li><li><a name="kedro.extras.datasets.pandas.sql_dataset.SQLTableDataSet"></a><code><a href="kedro.extras.datasets.pandas.sql_dataset.SQLTableDataSet.html" class="internal-link">kedro.extras.datasets.pandas.sql_dataset.SQLTableDataSet</a></code> - <tt class="rst-docutils literal">SQLTableDataSet</tt> loads data from a SQL table and saves a pandas dataframe to a table. It uses <tt class="rst-docutils literal">pandas.DataFrame</tt> internally, so it supports all allowed pandas options on <tt class="rst-docutils literal">read_sql_table</tt> and <tt class="rst-docutils literal">to_sql</tt> methods. ...</li><li><a name="kedro.extras.datasets.redis.PickleDataSet"></a><code><a href="kedro.extras.datasets.redis.PickleDataSet.html" class="internal-link">kedro.extras.datasets.redis.PickleDataSet</a></code> - <tt class="rst-docutils literal">PickleDataSet</tt> loads/saves data from/to a Redis database. The underlying functionality is supported by the redis library, so it supports all allowed options for instantiating the redis app <tt class="rst-docutils literal">from_url</tt> and setting a value.</li><li><a name="kedro.extras.datasets.spark.DeltaTableDataSet"></a><code><a href="kedro.extras.datasets.spark.DeltaTableDataSet.html" class="internal-link">kedro.extras.datasets.spark.DeltaTableDataSet</a></code> - <tt class="rst-docutils literal">DeltaTableDataSet</tt> loads data into DeltaTable objects.</li><li><a name="kedro.extras.datasets.spark.spark_jdbc_dataset.SparkJDBCDataSet"></a><code><a href="kedro.extras.datasets.spark.spark_jdbc_dataset.SparkJDBCDataSet.html" class="internal-link">kedro.extras.datasets.spark.spark_jdbc_dataset.SparkJDBCDataSet</a></code> - <tt class="rst-docutils literal">SparkJDBCDataSet</tt> loads data from a database table accessible via JDBC URL url and connection properties and saves the content of a PySpark DataFrame to an external database table via JDBC.  It uses <tt class="rst-docutils literal">pyspark.sql.DataFrameReader</tt>...</li><li><a name="kedro.extras.datasets.spark.SparkHiveDataSet"></a><code><a href="kedro.extras.datasets.spark.SparkHiveDataSet.html" class="internal-link">kedro.extras.datasets.spark.SparkHiveDataSet</a></code> - <tt class="rst-docutils literal">SparkHiveDataSet</tt> loads and saves Spark dataframes stored on Hive. This data set also handles some incompatible file types such as using partitioned parquet on hive which will not normally allow upserts to existing data without a complete replacement of the existing file/partition.</li><li><a name="kedro.extras.datasets.video.VideoDataSet"></a><code><a href="kedro.extras.datasets.video.VideoDataSet.html" class="internal-link">kedro.extras.datasets.video.VideoDataSet</a></code> - <tt class="rst-docutils literal">VideoDataSet</tt> loads / save video data from a given filepath as sequence of PIL.Image.Image using OpenCV.</li><li><a name="kedro.io.AbstractVersionedDataSet"></a><code><a href="kedro.io.AbstractVersionedDataSet.html" class="internal-link">kedro.io.AbstractVersionedDataSet</a></code> - <tt class="rst-docutils literal">AbstractVersionedDataSet</tt> is the base class for all versioned data set implementations. All data sets that implement versioning should extend this abstract class and implement the methods marked as abstract.<ul><li><a name="kedro.extras.datasets.email.EmailMessageDataSet"></a><code><a href="kedro.extras.datasets.email.EmailMessageDataSet.html" class="internal-link">kedro.extras.datasets.email.EmailMessageDataSet</a></code> - <tt class="rst-docutils literal">EmailMessageDataSet</tt> loads/saves an email message from/to a file using an underlying filesystem (e.g.: local, S3, GCS). It uses the <tt class="rst-docutils literal">email</tt> package in the standard library to manage email messages.</li><li><a name="kedro.extras.datasets.geopandas.GeoJSONDataSet"></a><code><a href="kedro.extras.datasets.geopandas.GeoJSONDataSet.html" class="internal-link">kedro.extras.datasets.geopandas.GeoJSONDataSet</a></code> - <tt class="rst-docutils literal">GeoJSONDataSet</tt> loads/saves data to a GeoJSON file using an underlying filesystem (eg: local, S3, GCS). The underlying functionality is supported by geopandas, so it supports all allowed geopandas (pandas) options for loading and saving GeoJSON files.</li><li><a name="kedro.extras.datasets.holoviews.HoloviewsWriter"></a><code><a href="kedro.extras.datasets.holoviews.HoloviewsWriter.html" class="internal-link">kedro.extras.datasets.holoviews.HoloviewsWriter</a></code> - <tt class="rst-docutils literal">HoloviewsWriter</tt> saves Holoviews objects to image file(s) in an underlying filesystem (e.g. local, S3, GCS).</li><li><a name="kedro.extras.datasets.json.JSONDataSet"></a><code><a href="kedro.extras.datasets.json.JSONDataSet.html" class="internal-link">kedro.extras.datasets.json.JSONDataSet</a></code> - <tt class="rst-docutils literal">JSONDataSet</tt> loads/saves data from/to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). It uses native json to handle the JSON file.<ul><li><a name="kedro.extras.datasets.tracking.JSONDataSet"></a><code><a href="kedro.extras.datasets.tracking.JSONDataSet.html" class="internal-link">kedro.extras.datasets.tracking.JSONDataSet</a></code> - <tt class="rst-docutils literal">JSONDataSet</tt> saves data to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). It uses native json to handle the JSON file. The <tt class="rst-docutils literal">JSONDataSet</tt> is part of Kedro Experiment Tracking. The dataset is write-only and it is versioned by default.</li><li><a name="kedro.extras.datasets.tracking.MetricsDataSet"></a><code><a href="kedro.extras.datasets.tracking.MetricsDataSet.html" class="internal-link">kedro.extras.datasets.tracking.MetricsDataSet</a></code> - <tt class="rst-docutils literal">MetricsDataSet</tt> saves data to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). It uses native json to handle the JSON file. The <tt class="rst-docutils literal">MetricsDataSet</tt> is part of Kedro Experiment Tracking. The dataset is write-only, it is versioned by default and only takes metrics of numeric values.</li></ul></li><li><a name="kedro.extras.datasets.matplotlib.MatplotlibWriter"></a><code><a href="kedro.extras.datasets.matplotlib.MatplotlibWriter.html" class="internal-link">kedro.extras.datasets.matplotlib.MatplotlibWriter</a></code> - <tt class="rst-docutils literal">MatplotlibWriter</tt> saves one or more Matplotlib objects as image files to an underlying filesystem (e.g. local, S3, GCS).</li><li><a name="kedro.extras.datasets.networkx.GMLDataSet"></a><code><a href="kedro.extras.datasets.networkx.GMLDataSet.html" class="internal-link">kedro.extras.datasets.networkx.GMLDataSet</a></code> - <tt class="rst-docutils literal">GMLDataSet</tt> loads and saves graphs to a GML file using an underlying filesystem (e.g.: local, S3, GCS). <tt class="rst-docutils literal">NetworkX</tt> is used to create GML data. See <a class="rst-reference external" href="https://networkx.org/documentation/stable/tutorial.html" target="_top">https://networkx.org/documentation/stable/tutorial.html</a> for details.</li><li><a name="kedro.extras.datasets.networkx.GraphMLDataSet"></a><code><a href="kedro.extras.datasets.networkx.GraphMLDataSet.html" class="internal-link">kedro.extras.datasets.networkx.GraphMLDataSet</a></code> - <tt class="rst-docutils literal">GraphMLDataSet</tt> loads and saves graphs to a GraphML file using an underlying filesystem (e.g.: local, S3, GCS). <tt class="rst-docutils literal">NetworkX</tt> is used to create GraphML data. See <a class="rst-reference external" href="https://networkx.org/documentation/stable/tutorial.html" target="_top">https://networkx.org/documentation/stable/tutorial.html</a>...</li><li><a name="kedro.extras.datasets.networkx.JSONDataSet"></a><code><a href="kedro.extras.datasets.networkx.JSONDataSet.html" class="internal-link">kedro.extras.datasets.networkx.JSONDataSet</a></code> - NetworkX <tt class="rst-docutils literal">JSONDataSet</tt> loads and saves graphs to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). <tt class="rst-docutils literal">NetworkX</tt> is used to create JSON data. See <a class="rst-reference external" href="https://networkx.org/documentation/stable/tutorial.html" target="_top">https://networkx.org/documentation/stable/tutorial.html</a>...</li><li><a name="kedro.extras.datasets.pandas.CSVDataSet"></a><code><a href="kedro.extras.datasets.pandas.CSVDataSet.html" class="internal-link">kedro.extras.datasets.pandas.CSVDataSet</a></code> - <tt class="rst-docutils literal">CSVDataSet</tt> loads/saves data from/to a CSV file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to handle the CSV file.</li><li><a name="kedro.extras.datasets.pandas.ExcelDataSet"></a><code><a href="kedro.extras.datasets.pandas.ExcelDataSet.html" class="internal-link">kedro.extras.datasets.pandas.ExcelDataSet</a></code> - <tt class="rst-docutils literal">ExcelDataSet</tt> loads/saves data from/to a Excel file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to handle the Excel file.</li><li><a name="kedro.extras.datasets.pandas.FeatherDataSet"></a><code><a href="kedro.extras.datasets.pandas.FeatherDataSet.html" class="internal-link">kedro.extras.datasets.pandas.FeatherDataSet</a></code> - <tt class="rst-docutils literal">FeatherDataSet</tt> loads and saves data to a feather file using an underlying filesystem (e.g.: local, S3, GCS). The underlying functionality is supported by pandas, so it supports all allowed pandas options for loading and saving csv files.</li><li><a name="kedro.extras.datasets.pandas.GenericDataSet"></a><code><a href="kedro.extras.datasets.pandas.GenericDataSet.html" class="internal-link">kedro.extras.datasets.pandas.GenericDataSet</a></code> - <code><a href="kedro.extras.datasets.pandas.GenericDataSet.html" class="internal-link" title="kedro.extras.datasets.pandas.GenericDataSet">pandas.GenericDataSet</a></code> loads/saves data from/to a data file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to dynamically select the appropriate type of read/write target on a best effort basis.</li><li><a name="kedro.extras.datasets.pandas.HDFDataSet"></a><code><a href="kedro.extras.datasets.pandas.HDFDataSet.html" class="internal-link">kedro.extras.datasets.pandas.HDFDataSet</a></code> - <tt class="rst-docutils literal">HDFDataSet</tt> loads/saves data from/to a hdf file using an underlying filesystem (e.g. local, S3, GCS). It uses pandas.HDFStore to handle the hdf file.</li><li><a name="kedro.extras.datasets.pandas.JSONDataSet"></a><code><a href="kedro.extras.datasets.pandas.JSONDataSet.html" class="internal-link">kedro.extras.datasets.pandas.JSONDataSet</a></code> - <tt class="rst-docutils literal">JSONDataSet</tt> loads/saves data from/to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to handle the json file.</li><li><a name="kedro.extras.datasets.pandas.ParquetDataSet"></a><code><a href="kedro.extras.datasets.pandas.ParquetDataSet.html" class="internal-link">kedro.extras.datasets.pandas.ParquetDataSet</a></code> - <tt class="rst-docutils literal">ParquetDataSet</tt> loads/saves data from/to a Parquet file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to handle the Parquet file.</li><li><a name="kedro.extras.datasets.pandas.XMLDataSet"></a><code><a href="kedro.extras.datasets.pandas.XMLDataSet.html" class="internal-link">kedro.extras.datasets.pandas.XMLDataSet</a></code> - <tt class="rst-docutils literal">XMLDataSet</tt> loads/saves data from/to a XML file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to handle the XML file.</li><li><a name="kedro.extras.datasets.pickle.PickleDataSet"></a><code><a href="kedro.extras.datasets.pickle.PickleDataSet.html" class="internal-link">kedro.extras.datasets.pickle.PickleDataSet</a></code> - <tt class="rst-docutils literal">PickleDataSet</tt> loads/saves data from/to a Pickle file using an underlying filesystem (e.g.: local, S3, GCS). The underlying functionality is supported by the specified backend library passed in (defaults to the ...</li><li><a name="kedro.extras.datasets.pillow.ImageDataSet"></a><code><a href="kedro.extras.datasets.pillow.ImageDataSet.html" class="internal-link">kedro.extras.datasets.pillow.ImageDataSet</a></code> - <tt class="rst-docutils literal">ImageDataSet</tt> loads/saves image data as <code><a href="https://pydocbrowser.github.io/numpy/latest/index.html" class="intersphinx-link">numpy</a></code> from an underlying filesystem (e.g.: local, S3, GCS). It uses Pillow to handle image file.</li><li><a name="kedro.extras.datasets.plotly.JSONDataSet"></a><code><a href="kedro.extras.datasets.plotly.JSONDataSet.html" class="internal-link">kedro.extras.datasets.plotly.JSONDataSet</a></code> - <tt class="rst-docutils literal">JSONDataSet</tt> loads/saves a plotly figure from/to a JSON file using an underlying filesystem (e.g.: local, S3, GCS).<ul><li><a name="kedro.extras.datasets.plotly.PlotlyDataSet"></a><code><a href="kedro.extras.datasets.plotly.PlotlyDataSet.html" class="internal-link">kedro.extras.datasets.plotly.PlotlyDataSet</a></code> - <tt class="rst-docutils literal">PlotlyDataSet</tt> generates a plot from a pandas DataFrame and saves it to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). It loads the JSON into a plotly figure.</li></ul></li><li><a name="kedro.extras.datasets.spark.SparkDataSet"></a><code><a href="kedro.extras.datasets.spark.SparkDataSet.html" class="internal-link">kedro.extras.datasets.spark.SparkDataSet</a></code> - <tt class="rst-docutils literal">SparkDataSet</tt> loads and saves Spark dataframes.</li><li><a name="kedro.extras.datasets.svmlight.SVMLightDataSet"></a><code><a href="kedro.extras.datasets.svmlight.SVMLightDataSet.html" class="internal-link">kedro.extras.datasets.svmlight.SVMLightDataSet</a></code> - <tt class="rst-docutils literal">SVMLightDataSet</tt> loads/saves data from/to a svmlight/libsvm file using an underlying filesystem (e.g.: local, S3, GCS). It uses sklearn functions <tt class="rst-docutils literal">dump_svmlight_file</tt> to save and <tt class="rst-docutils literal">load_svmlight_file</tt> to load a file.</li><li><a name="kedro.extras.datasets.tensorflow.TensorFlowModelDataset"></a><code><a href="kedro.extras.datasets.tensorflow.TensorFlowModelDataset.html" class="internal-link">kedro.extras.datasets.tensorflow.TensorFlowModelDataset</a></code> - <tt class="rst-docutils literal">TensorflowModelDataset</tt> loads and saves TensorFlow models. The underlying functionality is supported by, and passes input arguments through to, TensorFlow 2.X load_model and save_model methods.</li><li><a name="kedro.extras.datasets.text.TextDataSet"></a><code><a href="kedro.extras.datasets.text.TextDataSet.html" class="internal-link">kedro.extras.datasets.text.TextDataSet</a></code> - <tt class="rst-docutils literal">TextDataSet</tt> loads/saves data from/to a text file using an underlying filesystem (e.g.: local, S3, GCS)</li><li><a name="kedro.extras.datasets.yaml.YAMLDataSet"></a><code><a href="kedro.extras.datasets.yaml.YAMLDataSet.html" class="internal-link">kedro.extras.datasets.yaml.YAMLDataSet</a></code> - <tt class="rst-docutils literal">YAMLDataSet</tt> loads/saves data from/to a YAML file using an underlying filesystem (e.g.: local, S3, GCS). It uses PyYAML to handle the YAML file.</li></ul></li><li><a name="kedro.io.CachedDataSet"></a><code><a href="kedro.io.CachedDataSet.html" class="internal-link">kedro.io.CachedDataSet</a></code> - <tt class="rst-docutils literal">CachedDataSet</tt> is a dataset wrapper which caches in memory the data saved, so that the user avoids io operations with slow storage media.</li><li><a name="kedro.io.LambdaDataSet"></a><code><a href="kedro.io.LambdaDataSet.html" class="internal-link">kedro.io.LambdaDataSet</a></code> - <tt class="rst-docutils literal">LambdaDataSet</tt> loads and saves data to a data set. It relies on delegating to specific implementation such as csv, sql, etc.</li><li><a name="kedro.io.MemoryDataSet"></a><code><a href="kedro.io.MemoryDataSet.html" class="internal-link">kedro.io.MemoryDataSet</a></code> - <tt class="rst-docutils literal">MemoryDataSet</tt> loads and saves data from/to an in-memory Python object.</li><li><a name="kedro.io.PartitionedDataSet"></a><code><a href="kedro.io.PartitionedDataSet.html" class="internal-link">kedro.io.PartitionedDataSet</a></code> - <tt class="rst-docutils literal">PartitionedDataSet</tt> loads and saves partitioned file-like data using the underlying dataset definition. For filesystem level operations it uses <code>fsspec</code>: <a class="rst-reference external" href="https://github.com/intake/filesystem_spec" target="_top">https://github.com/intake/filesystem_spec</a>.<ul><li><a name="kedro.io.IncrementalDataSet"></a><code><a href="kedro.io.IncrementalDataSet.html" class="internal-link">kedro.io.IncrementalDataSet</a></code> - <tt class="rst-docutils literal">IncrementalDataSet</tt> inherits from <tt class="rst-docutils literal">PartitionedDataSet</tt>, which loads and saves partitioned file-like data using the underlying dataset definition. For filesystem level operations it uses <code>fsspec</code>: <a class="rst-reference external" href="https://github.com/intake/filesystem_spec" target="_top">https://github.com/intake/filesystem_spec</a>...</li></ul></li></ul></li><li><code><a href="kedro.io.AbstractVersionedDataSet.html" class="internal-link">kedro.io.AbstractVersionedDataSet</a></code> - <tt class="rst-docutils literal">AbstractVersionedDataSet</tt> is the base class for all versioned data set implementations. All data sets that implement versioning should extend this abstract class and implement the methods marked as abstract.<ul><li><code><a href="kedro.extras.datasets.email.EmailMessageDataSet.html" class="internal-link">kedro.extras.datasets.email.EmailMessageDataSet</a></code> - <tt class="rst-docutils literal">EmailMessageDataSet</tt> loads/saves an email message from/to a file using an underlying filesystem (e.g.: local, S3, GCS). It uses the <tt class="rst-docutils literal">email</tt> package in the standard library to manage email messages.</li><li><code><a href="kedro.extras.datasets.geopandas.GeoJSONDataSet.html" class="internal-link">kedro.extras.datasets.geopandas.GeoJSONDataSet</a></code> - <tt class="rst-docutils literal">GeoJSONDataSet</tt> loads/saves data to a GeoJSON file using an underlying filesystem (eg: local, S3, GCS). The underlying functionality is supported by geopandas, so it supports all allowed geopandas (pandas) options for loading and saving GeoJSON files.</li><li><code><a href="kedro.extras.datasets.holoviews.HoloviewsWriter.html" class="internal-link">kedro.extras.datasets.holoviews.HoloviewsWriter</a></code> - <tt class="rst-docutils literal">HoloviewsWriter</tt> saves Holoviews objects to image file(s) in an underlying filesystem (e.g. local, S3, GCS).</li><li><code><a href="kedro.extras.datasets.json.JSONDataSet.html" class="internal-link">kedro.extras.datasets.json.JSONDataSet</a></code> - <tt class="rst-docutils literal">JSONDataSet</tt> loads/saves data from/to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). It uses native json to handle the JSON file.<ul><li><code><a href="kedro.extras.datasets.tracking.JSONDataSet.html" class="internal-link">kedro.extras.datasets.tracking.JSONDataSet</a></code> - <tt class="rst-docutils literal">JSONDataSet</tt> saves data to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). It uses native json to handle the JSON file. The <tt class="rst-docutils literal">JSONDataSet</tt> is part of Kedro Experiment Tracking. The dataset is write-only and it is versioned by default.</li><li><code><a href="kedro.extras.datasets.tracking.MetricsDataSet.html" class="internal-link">kedro.extras.datasets.tracking.MetricsDataSet</a></code> - <tt class="rst-docutils literal">MetricsDataSet</tt> saves data to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). It uses native json to handle the JSON file. The <tt class="rst-docutils literal">MetricsDataSet</tt> is part of Kedro Experiment Tracking. The dataset is write-only, it is versioned by default and only takes metrics of numeric values.</li></ul></li><li><code><a href="kedro.extras.datasets.matplotlib.MatplotlibWriter.html" class="internal-link">kedro.extras.datasets.matplotlib.MatplotlibWriter</a></code> - <tt class="rst-docutils literal">MatplotlibWriter</tt> saves one or more Matplotlib objects as image files to an underlying filesystem (e.g. local, S3, GCS).</li><li><code><a href="kedro.extras.datasets.networkx.GMLDataSet.html" class="internal-link">kedro.extras.datasets.networkx.GMLDataSet</a></code> - <tt class="rst-docutils literal">GMLDataSet</tt> loads and saves graphs to a GML file using an underlying filesystem (e.g.: local, S3, GCS). <tt class="rst-docutils literal">NetworkX</tt> is used to create GML data. See <a class="rst-reference external" href="https://networkx.org/documentation/stable/tutorial.html" target="_top">https://networkx.org/documentation/stable/tutorial.html</a> for details.</li><li><code><a href="kedro.extras.datasets.networkx.GraphMLDataSet.html" class="internal-link">kedro.extras.datasets.networkx.GraphMLDataSet</a></code> - <tt class="rst-docutils literal">GraphMLDataSet</tt> loads and saves graphs to a GraphML file using an underlying filesystem (e.g.: local, S3, GCS). <tt class="rst-docutils literal">NetworkX</tt> is used to create GraphML data. See <a class="rst-reference external" href="https://networkx.org/documentation/stable/tutorial.html" target="_top">https://networkx.org/documentation/stable/tutorial.html</a>...</li><li><code><a href="kedro.extras.datasets.networkx.JSONDataSet.html" class="internal-link">kedro.extras.datasets.networkx.JSONDataSet</a></code> - NetworkX <tt class="rst-docutils literal">JSONDataSet</tt> loads and saves graphs to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). <tt class="rst-docutils literal">NetworkX</tt> is used to create JSON data. See <a class="rst-reference external" href="https://networkx.org/documentation/stable/tutorial.html" target="_top">https://networkx.org/documentation/stable/tutorial.html</a>...</li><li><code><a href="kedro.extras.datasets.pandas.CSVDataSet.html" class="internal-link">kedro.extras.datasets.pandas.CSVDataSet</a></code> - <tt class="rst-docutils literal">CSVDataSet</tt> loads/saves data from/to a CSV file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to handle the CSV file.</li><li><code><a href="kedro.extras.datasets.pandas.ExcelDataSet.html" class="internal-link">kedro.extras.datasets.pandas.ExcelDataSet</a></code> - <tt class="rst-docutils literal">ExcelDataSet</tt> loads/saves data from/to a Excel file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to handle the Excel file.</li><li><code><a href="kedro.extras.datasets.pandas.FeatherDataSet.html" class="internal-link">kedro.extras.datasets.pandas.FeatherDataSet</a></code> - <tt class="rst-docutils literal">FeatherDataSet</tt> loads and saves data to a feather file using an underlying filesystem (e.g.: local, S3, GCS). The underlying functionality is supported by pandas, so it supports all allowed pandas options for loading and saving csv files.</li><li><code><a href="kedro.extras.datasets.pandas.GenericDataSet.html" class="internal-link">kedro.extras.datasets.pandas.GenericDataSet</a></code> - <code><a href="kedro.extras.datasets.pandas.GenericDataSet.html" class="internal-link" title="kedro.extras.datasets.pandas.GenericDataSet">pandas.GenericDataSet</a></code> loads/saves data from/to a data file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to dynamically select the appropriate type of read/write target on a best effort basis.</li><li><code><a href="kedro.extras.datasets.pandas.HDFDataSet.html" class="internal-link">kedro.extras.datasets.pandas.HDFDataSet</a></code> - <tt class="rst-docutils literal">HDFDataSet</tt> loads/saves data from/to a hdf file using an underlying filesystem (e.g. local, S3, GCS). It uses pandas.HDFStore to handle the hdf file.</li><li><code><a href="kedro.extras.datasets.pandas.JSONDataSet.html" class="internal-link">kedro.extras.datasets.pandas.JSONDataSet</a></code> - <tt class="rst-docutils literal">JSONDataSet</tt> loads/saves data from/to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to handle the json file.</li><li><code><a href="kedro.extras.datasets.pandas.ParquetDataSet.html" class="internal-link">kedro.extras.datasets.pandas.ParquetDataSet</a></code> - <tt class="rst-docutils literal">ParquetDataSet</tt> loads/saves data from/to a Parquet file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to handle the Parquet file.</li><li><code><a href="kedro.extras.datasets.pandas.XMLDataSet.html" class="internal-link">kedro.extras.datasets.pandas.XMLDataSet</a></code> - <tt class="rst-docutils literal">XMLDataSet</tt> loads/saves data from/to a XML file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to handle the XML file.</li><li><code><a href="kedro.extras.datasets.pickle.PickleDataSet.html" class="internal-link">kedro.extras.datasets.pickle.PickleDataSet</a></code> - <tt class="rst-docutils literal">PickleDataSet</tt> loads/saves data from/to a Pickle file using an underlying filesystem (e.g.: local, S3, GCS). The underlying functionality is supported by the specified backend library passed in (defaults to the ...</li><li><code><a href="kedro.extras.datasets.pillow.ImageDataSet.html" class="internal-link">kedro.extras.datasets.pillow.ImageDataSet</a></code> - <tt class="rst-docutils literal">ImageDataSet</tt> loads/saves image data as <code><a href="https://pydocbrowser.github.io/numpy/latest/index.html" class="intersphinx-link">numpy</a></code> from an underlying filesystem (e.g.: local, S3, GCS). It uses Pillow to handle image file.</li><li><code><a href="kedro.extras.datasets.plotly.JSONDataSet.html" class="internal-link">kedro.extras.datasets.plotly.JSONDataSet</a></code> - <tt class="rst-docutils literal">JSONDataSet</tt> loads/saves a plotly figure from/to a JSON file using an underlying filesystem (e.g.: local, S3, GCS).<ul><li><code><a href="kedro.extras.datasets.plotly.PlotlyDataSet.html" class="internal-link">kedro.extras.datasets.plotly.PlotlyDataSet</a></code> - <tt class="rst-docutils literal">PlotlyDataSet</tt> generates a plot from a pandas DataFrame and saves it to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). It loads the JSON into a plotly figure.</li></ul></li><li><code><a href="kedro.extras.datasets.spark.SparkDataSet.html" class="internal-link">kedro.extras.datasets.spark.SparkDataSet</a></code> - <tt class="rst-docutils literal">SparkDataSet</tt> loads and saves Spark dataframes.</li><li><code><a href="kedro.extras.datasets.svmlight.SVMLightDataSet.html" class="internal-link">kedro.extras.datasets.svmlight.SVMLightDataSet</a></code> - <tt class="rst-docutils literal">SVMLightDataSet</tt> loads/saves data from/to a svmlight/libsvm file using an underlying filesystem (e.g.: local, S3, GCS). It uses sklearn functions <tt class="rst-docutils literal">dump_svmlight_file</tt> to save and <tt class="rst-docutils literal">load_svmlight_file</tt> to load a file.</li><li><code><a href="kedro.extras.datasets.tensorflow.TensorFlowModelDataset.html" class="internal-link">kedro.extras.datasets.tensorflow.TensorFlowModelDataset</a></code> - <tt class="rst-docutils literal">TensorflowModelDataset</tt> loads and saves TensorFlow models. The underlying functionality is supported by, and passes input arguments through to, TensorFlow 2.X load_model and save_model methods.</li><li><code><a href="kedro.extras.datasets.text.TextDataSet.html" class="internal-link">kedro.extras.datasets.text.TextDataSet</a></code> - <tt class="rst-docutils literal">TextDataSet</tt> loads/saves data from/to a text file using an underlying filesystem (e.g.: local, S3, GCS)</li><li><code><a href="kedro.extras.datasets.yaml.YAMLDataSet.html" class="internal-link">kedro.extras.datasets.yaml.YAMLDataSet</a></code> - <tt class="rst-docutils literal">YAMLDataSet</tt> loads/saves data from/to a YAML file using an underlying filesystem (e.g.: local, S3, GCS). It uses PyYAML to handle the YAML file.</li></ul></li><li><a name="kedro.runner.AbstractRunner"></a><code><a href="kedro.runner.AbstractRunner.html" class="internal-link">kedro.runner.AbstractRunner</a></code> - <tt class="rst-docutils literal">AbstractRunner</tt> is the base class for all <tt class="rst-docutils literal">Pipeline</tt> runner implementations.<ul><li><a name="kedro.runner.ParallelRunner"></a><code><a href="kedro.runner.ParallelRunner.html" class="internal-link">kedro.runner.ParallelRunner</a></code> - <tt class="rst-docutils literal">ParallelRunner</tt> is an <tt class="rst-docutils literal">AbstractRunner</tt> implementation. It can be used to run the <tt class="rst-docutils literal">Pipeline</tt> in parallel groups formed by toposort. Please note that this <code><a href="kedro.runner.runner.html" class="internal-link" title="kedro.runner.runner">runner</a></code> implementation validates dataset using the <tt class="rst-docutils literal">_validate_catalog</tt>...</li><li><a name="kedro.runner.SequentialRunner"></a><code><a href="kedro.runner.SequentialRunner.html" class="internal-link">kedro.runner.SequentialRunner</a></code> - <tt class="rst-docutils literal">SequentialRunner</tt> is an <tt class="rst-docutils literal">AbstractRunner</tt> implementation. It can be used to run the <tt class="rst-docutils literal">Pipeline</tt> in a sequential manner using a topological sort of provided nodes.</li><li><a name="kedro.runner.ThreadRunner"></a><code><a href="kedro.runner.ThreadRunner.html" class="internal-link">kedro.runner.ThreadRunner</a></code> - <tt class="rst-docutils literal">ThreadRunner</tt> is an <tt class="rst-docutils literal">AbstractRunner</tt> implementation. It can be used to run the <tt class="rst-docutils literal">Pipeline</tt> in parallel groups formed by toposort using threads.</li></ul></li></ul></li><li><code>click.CommandCollection</code><ul><li><a name="kedro.framework.cli.utils.CommandCollection"></a><code><a href="kedro.framework.cli.utils.CommandCollection.html" class="internal-link">kedro.framework.cli.utils.CommandCollection</a></code> - Modified from the Click one to still run the source groups function.<ul><li><a name="kedro.framework.cli.cli.KedroCLI"></a><code><a href="kedro.framework.cli.cli.KedroCLI.html" class="internal-link">kedro.framework.cli.cli.KedroCLI</a></code> - A CommandCollection class to encapsulate the KedroCLI command loading.</li></ul></li></ul></li><li><code>click.exceptions.ClickException</code><ul><li><a name="kedro.framework.cli.utils.KedroCliError"></a><code><a href="kedro.framework.cli.utils.KedroCliError.html" class="internal-link">kedro.framework.cli.utils.KedroCliError</a></code> - Exceptions generated from the Kedro CLI.</li></ul></li><li class="private"><code>collections.abc.MutableMapping</code><ul><li class="private"><a name="kedro.framework.project._ProjectPipelines"></a><code><a href="kedro.framework.project._ProjectPipelines.html" class="internal-link">kedro.framework.project._ProjectPipelines</a></code> - A read-only lazy dictionary-like object to hold the project pipelines. On configure it will store the pipelines module. On first data access, e.g. through __getitem__, it will load the registered pipelines and merge them with pipelines defined from hooks.</li></ul></li><li><code>collections.abc.Sequence</code><ul><li><a name="kedro.extras.datasets.video.video_dataset.AbstractVideo"></a><code><a href="kedro.extras.datasets.video.video_dataset.AbstractVideo.html" class="internal-link">kedro.extras.datasets.video.video_dataset.AbstractVideo</a></code> - Base class for the underlying video data<ul><li><a name="kedro.extras.datasets.video.video_dataset.FileVideo"></a><code><a href="kedro.extras.datasets.video.video_dataset.FileVideo.html" class="internal-link">kedro.extras.datasets.video.video_dataset.FileVideo</a></code> - A video object read from a file</li><li><a name="kedro.extras.datasets.video.video_dataset.GeneratorVideo"></a><code><a href="kedro.extras.datasets.video.video_dataset.GeneratorVideo.html" class="internal-link">kedro.extras.datasets.video.video_dataset.GeneratorVideo</a></code> - A video object with frames yielded by a generator</li><li><a name="kedro.extras.datasets.video.video_dataset.SequenceVideo"></a><code><a href="kedro.extras.datasets.video.video_dataset.SequenceVideo.html" class="internal-link">kedro.extras.datasets.video.video_dataset.SequenceVideo</a></code> - A video object read from an indexable sequence of frames</li></ul></li></ul></li><li><code>collections.UserDict</code><ul><li><a name="kedro.config.AbstractConfigLoader"></a><code><a href="kedro.config.AbstractConfigLoader.html" class="internal-link">kedro.config.AbstractConfigLoader</a></code> - for all <code><a href="kedro.config.ConfigLoader.html" class="internal-link" title="kedro.config.ConfigLoader">ConfigLoader</a></code> implementations.<ul><li><a name="kedro.config.ConfigLoader"></a><code><a href="kedro.config.ConfigLoader.html" class="internal-link">kedro.config.ConfigLoader</a></code> - Recursively scan directories (config paths) contained in <tt class="rst-docutils literal">conf_source</tt> for configuration files with a <tt class="rst-docutils literal">yaml</tt>, <tt class="rst-docutils literal">yml</tt>, <tt class="rst-docutils literal">json</tt>, <tt class="rst-docutils literal">ini</tt>, <tt class="rst-docutils literal">pickle</tt>, <tt class="rst-docutils literal">xml</tt> or <tt class="rst-docutils literal">properties</tt> extension, load them, and return them in the form of a config dictionary.</li><li><a name="kedro.config.OmegaConfigLoader"></a><code><a href="kedro.config.OmegaConfigLoader.html" class="internal-link">kedro.config.OmegaConfigLoader</a></code> - Recursively scan directories (config paths) contained in <tt class="rst-docutils literal">conf_source</tt> for configuration files with a <tt class="rst-docutils literal">yaml</tt>, <tt class="rst-docutils literal">yml</tt> or <tt class="rst-docutils literal">json</tt> extension, load and merge them through <tt class="rst-docutils literal">OmegaConf</tt> (<a class="rst-reference external" href="https://omegaconf.readthedocs.io/" target="_top">https://omegaconf.readthedocs.io/</a>...</li><li><a name="kedro.config.TemplatedConfigLoader"></a><code><a href="kedro.config.TemplatedConfigLoader.html" class="internal-link">kedro.config.TemplatedConfigLoader</a></code> - Extension of the <tt class="rst-docutils literal">ConfigLoader</tt> class that allows for template values, wrapped in brackets like: ${...}, to be automatically formatted based on the configs.</li></ul></li><li class="private"><a name="kedro.framework.project._ProjectLogging"></a><code><a href="kedro.framework.project._ProjectLogging.html" class="internal-link">kedro.framework.project._ProjectLogging</a></code> - <span class="undocumented">No class docstring; 0/1 instance variable, 2/2 methods documented</span></li><li><a name="kedro.framework.session.store.BaseSessionStore"></a><code><a href="kedro.framework.session.store.BaseSessionStore.html" class="internal-link">kedro.framework.session.store.BaseSessionStore</a></code> - <tt class="rst-docutils literal">BaseSessionStore</tt> is the base class for all session stores. <tt class="rst-docutils literal">BaseSessionStore</tt> is an ephemeral store implementation that doesn't persist the session data.<ul><li><a name="kedro.framework.session.shelvestore.ShelveStore"></a><code><a href="kedro.framework.session.shelvestore.ShelveStore.html" class="internal-link">kedro.framework.session.shelvestore.ShelveStore</a></code> - Stores the session data on disk using <code><a href="https://docs.python.org/3/library/shelve.html#module-shelve" class="intersphinx-link">shelve</a></code> package. This is an example of how to persist data on disk.</li></ul></li></ul></li><li class="private"><code>dynaconf.LazySettings</code><ul><li class="private"><a name="kedro.framework.project._ProjectSettings"></a><code><a href="kedro.framework.project._ProjectSettings.html" class="internal-link">kedro.framework.project._ProjectSettings</a></code> - Define all settings available for users to configure in Kedro, along with their validation rules and default values. Use Dynaconf's LazySettings as base.</li></ul></li><li class="private"><code>dynaconf.validator.Validator</code><ul><li class="private"><a name="kedro.framework.project._HasSharedParentClassValidator"></a><code><a href="kedro.framework.project._HasSharedParentClassValidator.html" class="internal-link">kedro.framework.project._HasSharedParentClassValidator</a></code> - A validator to check that the parent of the default class is an ancestor of the settings value.</li><li class="private"><a name="kedro.framework.project._IsSubclassValidator"></a><code><a href="kedro.framework.project._IsSubclassValidator.html" class="internal-link">kedro.framework.project._IsSubclassValidator</a></code> - A validator to check if the supplied setting value is a subclass of the default class</li></ul></li><li><code>Exception</code><ul><li><a name="kedro.config.BadConfigException"></a><code><a href="kedro.config.BadConfigException.html" class="internal-link">kedro.config.BadConfigException</a></code> - Raised when a configuration file cannot be loaded, for instance due to wrong syntax or poor formatting.</li><li><a name="kedro.config.MissingConfigException"></a><code><a href="kedro.config.MissingConfigException.html" class="internal-link">kedro.config.MissingConfigException</a></code> - Raised when no configuration files can be found within a config path</li><li><a name="kedro.framework.context.KedroContextError"></a><code><a href="kedro.framework.context.KedroContextError.html" class="internal-link">kedro.framework.context.KedroContextError</a></code> - Error occurred when loading project and running context pipeline.</li><li><a name="kedro.framework.session.session.KedroSessionError"></a><code><a href="kedro.framework.session.session.KedroSessionError.html" class="internal-link">kedro.framework.session.session.KedroSessionError</a></code> - <tt class="rst-docutils literal">KedroSessionError</tt> raised by <tt class="rst-docutils literal">KedroSession</tt> in the case that multiple runs are attempted in one session.</li><li><a name="kedro.io.DataSetError"></a><code><a href="kedro.io.DataSetError.html" class="internal-link">kedro.io.DataSetError</a></code> - <tt class="rst-docutils literal">DataSetError</tt> raised by <tt class="rst-docutils literal">AbstractDataSet</tt> implementations in case of failure of input/output methods.<ul><li><a name="kedro.io.core.VersionNotFoundError"></a><code><a href="kedro.io.core.VersionNotFoundError.html" class="internal-link">kedro.io.core.VersionNotFoundError</a></code> - <tt class="rst-docutils literal">VersionNotFoundError</tt> raised by <tt class="rst-docutils literal">AbstractVersionedDataSet</tt> implementations in case of no load versions available for the data set.</li><li><a name="kedro.io.DataSetAlreadyExistsError"></a><code><a href="kedro.io.DataSetAlreadyExistsError.html" class="internal-link">kedro.io.DataSetAlreadyExistsError</a></code> - <tt class="rst-docutils literal">DataSetAlreadyExistsError</tt> raised by <tt class="rst-docutils literal">DataCatalog</tt> class in case of trying to add a data set which already exists in the <tt class="rst-docutils literal">DataCatalog</tt>.</li><li><a name="kedro.io.DataSetNotFoundError"></a><code><a href="kedro.io.DataSetNotFoundError.html" class="internal-link">kedro.io.DataSetNotFoundError</a></code> - <tt class="rst-docutils literal">DataSetNotFoundError</tt> raised by <tt class="rst-docutils literal">DataCatalog</tt> class in case of trying to use a non-existing data set.</li></ul></li><li><a name="kedro.pipeline.modular_pipeline.ModularPipelineError"></a><code><a href="kedro.pipeline.modular_pipeline.ModularPipelineError.html" class="internal-link">kedro.pipeline.modular_pipeline.ModularPipelineError</a></code> - Raised when a modular pipeline is not adapted and integrated appropriately using the helper.</li><li><a name="kedro.pipeline.pipeline.CircularDependencyError"></a><code><a href="kedro.pipeline.pipeline.CircularDependencyError.html" class="internal-link">kedro.pipeline.pipeline.CircularDependencyError</a></code> - Raised when it is not possible to provide a topological execution order for nodes, due to a circular dependency existing in the node definition.</li><li><a name="kedro.pipeline.pipeline.ConfirmNotUniqueError"></a><code><a href="kedro.pipeline.pipeline.ConfirmNotUniqueError.html" class="internal-link">kedro.pipeline.pipeline.ConfirmNotUniqueError</a></code> - Raised when two or more nodes that are part of the same pipeline attempt to confirm the same dataset.</li><li><a name="kedro.pipeline.pipeline.OutputNotUniqueError"></a><code><a href="kedro.pipeline.pipeline.OutputNotUniqueError.html" class="internal-link">kedro.pipeline.pipeline.OutputNotUniqueError</a></code> - Raised when two or more nodes that are part of the same pipeline produce outputs with the same name.</li></ul></li><li><code>hdfs.InsecureClient</code><ul><li><a name="kedro.extras.datasets.spark.spark_dataset.KedroHdfsInsecureClient"></a><code><a href="kedro.extras.datasets.spark.spark_dataset.KedroHdfsInsecureClient.html" class="internal-link">kedro.extras.datasets.spark.spark_dataset.KedroHdfsInsecureClient</a></code> - Subclasses <tt class="rst-docutils literal">hdfs.InsecureClient</tt> and implements <tt class="rst-docutils literal">hdfs_exists</tt> and <tt class="rst-docutils literal">hdfs_glob</tt> methods required by <tt class="rst-docutils literal">SparkDataSet</tt></li></ul></li><li><a name="kedro.extras.datasets.video.video_dataset.SlicedVideo"></a><code><a href="kedro.extras.datasets.video.video_dataset.SlicedVideo.html" class="internal-link">kedro.extras.datasets.video.video_dataset.SlicedVideo</a></code> - A representation of slices of other video types</li><li><a name="kedro.framework.cli.hooks.specs.CLICommandSpecs"></a><code><a href="kedro.framework.cli.hooks.specs.CLICommandSpecs.html" class="internal-link">kedro.framework.cli.hooks.specs.CLICommandSpecs</a></code> - Namespace that defines all specifications for Kedro CLI's lifecycle hooks.</li><li class="private"><a name="kedro.framework.cli.starters._Prompt"></a><code><a href="kedro.framework.cli.starters._Prompt.html" class="internal-link">kedro.framework.cli.starters._Prompt</a></code> - Represent a single CLI prompt for <code>kedro new</code></li><li><a name="kedro.framework.cli.starters.KedroStarterSpec"></a><code><a href="kedro.framework.cli.starters.KedroStarterSpec.html" class="internal-link">kedro.framework.cli.starters.KedroStarterSpec</a></code> - Specification of custom kedro starter template</li><li><a name="kedro.framework.context.KedroContext"></a><code><a href="kedro.framework.context.KedroContext.html" class="internal-link">kedro.framework.context.KedroContext</a></code> - <tt class="rst-docutils literal">KedroContext</tt> is the base class which holds the configuration and Kedro's main functionality.</li><li class="private"><a name="kedro.framework.hooks.manager._NullPluginManager"></a><code><a href="kedro.framework.hooks.manager._NullPluginManager.html" class="internal-link">kedro.framework.hooks.manager._NullPluginManager</a></code> - This class creates an empty <tt class="rst-docutils literal">hook_manager</tt> that will ignore all calls to hooks, allowing the runner to function if no <tt class="rst-docutils literal">hook_manager</tt> has been instantiated.</li><li><a name="kedro.framework.hooks.specs.DataCatalogSpecs"></a><code><a href="kedro.framework.hooks.specs.DataCatalogSpecs.html" class="internal-link">kedro.framework.hooks.specs.DataCatalogSpecs</a></code> - Namespace that defines all specifications for a data catalog's lifecycle hooks.</li><li><a name="kedro.framework.hooks.specs.DatasetSpecs"></a><code><a href="kedro.framework.hooks.specs.DatasetSpecs.html" class="internal-link">kedro.framework.hooks.specs.DatasetSpecs</a></code> - Namespace that defines all specifications for a dataset's lifecycle hooks.</li><li><a name="kedro.framework.hooks.specs.KedroContextSpecs"></a><code><a href="kedro.framework.hooks.specs.KedroContextSpecs.html" class="internal-link">kedro.framework.hooks.specs.KedroContextSpecs</a></code> - Namespace that defines all specifications for a Kedro context's lifecycle hooks.</li><li><a name="kedro.framework.hooks.specs.NodeSpecs"></a><code><a href="kedro.framework.hooks.specs.NodeSpecs.html" class="internal-link">kedro.framework.hooks.specs.NodeSpecs</a></code> - Namespace that defines all specifications for a node's lifecycle hooks.</li><li><a name="kedro.framework.hooks.specs.PipelineSpecs"></a><code><a href="kedro.framework.hooks.specs.PipelineSpecs.html" class="internal-link">kedro.framework.hooks.specs.PipelineSpecs</a></code> - Namespace that defines all specifications for a pipeline's lifecycle hooks.</li><li><a name="kedro.framework.session.KedroSession"></a><code><a href="kedro.framework.session.KedroSession.html" class="internal-link">kedro.framework.session.KedroSession</a></code> - <tt class="rst-docutils literal">KedroSession</tt> is the object that is responsible for managing the lifecycle of a Kedro run. Use <code><a href="kedro.framework.session.KedroSession.html#create" class="internal-link" title="kedro.framework.session.KedroSession.create">KedroSession.create()</a></code> as a context manager to construct a new KedroSession with session data provided (see the example below).</li><li class="private"><a name="kedro.io.data_catalog._FrozenDatasets"></a><code><a href="kedro.io.data_catalog._FrozenDatasets.html" class="internal-link">kedro.io.data_catalog._FrozenDatasets</a></code> - Helper class to access underlying loaded datasets</li><li><a name="kedro.io.DataCatalog"></a><code><a href="kedro.io.DataCatalog.html" class="internal-link">kedro.io.DataCatalog</a></code> - <tt class="rst-docutils literal">DataCatalog</tt> stores instances of <tt class="rst-docutils literal">AbstractDataSet</tt> implementations to provide <tt class="rst-docutils literal">load</tt> and <tt class="rst-docutils literal">save</tt> capabilities from anywhere in the program. To use a <tt class="rst-docutils literal">DataCatalog</tt>, you need to instantiate it with a dictionary of data sets...</li><li><a name="kedro.pipeline.node.Node"></a><code><a href="kedro.pipeline.node.Node.html" class="internal-link">kedro.pipeline.node.Node</a></code> - <tt class="rst-docutils literal">Node</tt> is an auxiliary class facilitating the operations required to run user-provided functions as part of Kedro pipelines.</li><li><a name="kedro.pipeline.pipeline.Pipeline"></a><code><a href="kedro.pipeline.pipeline.Pipeline.html" class="internal-link">kedro.pipeline.pipeline.Pipeline</a></code> - A <tt class="rst-docutils literal">Pipeline</tt> defined as a collection of <tt class="rst-docutils literal">Node</tt> objects. This class treats nodes as part of a graph representation and provides inputs, outputs and execution order.</li><li class="private"><a name="kedro.runner.parallel_runner._SharedMemoryDataSet"></a><code><a href="kedro.runner.parallel_runner._SharedMemoryDataSet.html" class="internal-link">kedro.runner.parallel_runner._SharedMemoryDataSet</a></code> - <tt class="rst-docutils literal">_SharedMemoryDataSet</tt> is a wrapper class for a shared MemoryDataSet in SyncManager. It is not inherited from AbstractDataSet class.</li><li><code>logging.StreamHandler</code><ul><li><a name="kedro.extras.logging.ColorHandler"></a><code><a href="kedro.extras.logging.ColorHandler.html" class="internal-link">kedro.extras.logging.ColorHandler</a></code> - A color log handler.</li></ul></li><li><code>multiprocessing.managers.SyncManager</code><ul><li><a name="kedro.runner.parallel_runner.ParallelRunnerManager"></a><code><a href="kedro.runner.parallel_runner.ParallelRunnerManager.html" class="internal-link">kedro.runner.parallel_runner.ParallelRunnerManager</a></code> - <tt class="rst-docutils literal">ParallelRunnerManager</tt> is used to create shared <tt class="rst-docutils literal">MemoryDataSet</tt> objects as default data sets in a pipeline.</li></ul></li><li><code>namedtuple('Version', ['load', 'save'])</code><ul><li><a name="kedro.io.Version"></a><code><a href="kedro.io.Version.html" class="internal-link">kedro.io.Version</a></code> - This namedtuple is used to provide load and save versions for versioned data sets. If <tt class="rst-docutils literal">Version.load</tt> is None, then the latest available version is loaded. If <tt class="rst-docutils literal">Version.save</tt> is None, then save version is formatted as YYYY-MM-DDThh...</li></ul></li><li><code>pluggy.PluginManager</code><ul><li><a name="kedro.framework.cli.hooks.CLIHooksManager"></a><code><a href="kedro.framework.cli.hooks.CLIHooksManager.html" class="internal-link">kedro.framework.cli.hooks.CLIHooksManager</a></code> - Hooks manager to manage CLI hooks</li></ul></li><li><code>typing.Generic</code><ul><li><code><a href="kedro.io.AbstractDataSet.html" class="internal-link">kedro.io.AbstractDataSet</a></code> - <tt class="rst-docutils literal">AbstractDataSet</tt> is the base class for all data set implementations. All data set implementations should extend this abstract class and implement the methods marked as abstract. If a specific dataset implementation cannot be used in conjunction with the ...<ul><li><code><a href="kedro.extras.datasets.api.APIDataSet.html" class="internal-link">kedro.extras.datasets.api.APIDataSet</a></code> - <tt class="rst-docutils literal">APIDataSet</tt> loads the data from HTTP(S) APIs. It uses the python requests library: <a class="rst-reference external" href="https://requests.readthedocs.io/en/latest/" target="_top">https://requests.readthedocs.io/en/latest/</a></li><li><code><a href="kedro.extras.datasets.biosequence.BioSequenceDataSet.html" class="internal-link">kedro.extras.datasets.biosequence.BioSequenceDataSet</a></code> - <tt class="rst-docutils literal">BioSequenceDataSet</tt> loads and saves data to a sequence file.</li><li><code><a href="kedro.extras.datasets.dask.ParquetDataSet.html" class="internal-link">kedro.extras.datasets.dask.ParquetDataSet</a></code> - <tt class="rst-docutils literal">ParquetDataSet</tt> loads and saves data to parquet file(s). It uses Dask remote data services to handle the corresponding load and save operations: <a class="rst-reference external" href="https://docs.dask.org/en/latest/how-to/connect-to-remote-data.html" target="_top">https://docs.dask.org/en/latest/how-to/connect-to-remote-data.html</a>...</li><li><code><a href="kedro.extras.datasets.pandas.GBQQueryDataSet.html" class="internal-link">kedro.extras.datasets.pandas.GBQQueryDataSet</a></code> - <tt class="rst-docutils literal">GBQQueryDataSet</tt> loads data from a provided SQL query from Google BigQuery. It uses <tt class="rst-docutils literal">pandas.read_gbq</tt> which itself uses <tt class="rst-docutils literal"><span class="pre">pandas-gbq</span></tt> internally to read from BigQuery table. Therefore it supports all allowed pandas options on ...</li><li><code><a href="kedro.extras.datasets.pandas.GBQTableDataSet.html" class="internal-link">kedro.extras.datasets.pandas.GBQTableDataSet</a></code> - <tt class="rst-docutils literal">GBQTableDataSet</tt> loads and saves data from/to Google BigQuery. It uses pandas-gbq to read and write from/to BigQuery table.</li><li><code><a href="kedro.extras.datasets.pandas.sql_dataset.SQLQueryDataSet.html" class="internal-link">kedro.extras.datasets.pandas.sql_dataset.SQLQueryDataSet</a></code> - <tt class="rst-docutils literal">SQLQueryDataSet</tt> loads data from a provided SQL query. It uses <tt class="rst-docutils literal">pandas.DataFrame</tt> internally, so it supports all allowed pandas options on <tt class="rst-docutils literal">read_sql_query</tt>. Since Pandas uses SQLAlchemy behind the scenes, when instantiating ...</li><li><code><a href="kedro.extras.datasets.pandas.sql_dataset.SQLTableDataSet.html" class="internal-link">kedro.extras.datasets.pandas.sql_dataset.SQLTableDataSet</a></code> - <tt class="rst-docutils literal">SQLTableDataSet</tt> loads data from a SQL table and saves a pandas dataframe to a table. It uses <tt class="rst-docutils literal">pandas.DataFrame</tt> internally, so it supports all allowed pandas options on <tt class="rst-docutils literal">read_sql_table</tt> and <tt class="rst-docutils literal">to_sql</tt> methods. ...</li><li><code><a href="kedro.extras.datasets.redis.PickleDataSet.html" class="internal-link">kedro.extras.datasets.redis.PickleDataSet</a></code> - <tt class="rst-docutils literal">PickleDataSet</tt> loads/saves data from/to a Redis database. The underlying functionality is supported by the redis library, so it supports all allowed options for instantiating the redis app <tt class="rst-docutils literal">from_url</tt> and setting a value.</li><li><code><a href="kedro.extras.datasets.spark.DeltaTableDataSet.html" class="internal-link">kedro.extras.datasets.spark.DeltaTableDataSet</a></code> - <tt class="rst-docutils literal">DeltaTableDataSet</tt> loads data into DeltaTable objects.</li><li><code><a href="kedro.extras.datasets.spark.spark_jdbc_dataset.SparkJDBCDataSet.html" class="internal-link">kedro.extras.datasets.spark.spark_jdbc_dataset.SparkJDBCDataSet</a></code> - <tt class="rst-docutils literal">SparkJDBCDataSet</tt> loads data from a database table accessible via JDBC URL url and connection properties and saves the content of a PySpark DataFrame to an external database table via JDBC.  It uses <tt class="rst-docutils literal">pyspark.sql.DataFrameReader</tt>...</li><li><code><a href="kedro.extras.datasets.spark.SparkHiveDataSet.html" class="internal-link">kedro.extras.datasets.spark.SparkHiveDataSet</a></code> - <tt class="rst-docutils literal">SparkHiveDataSet</tt> loads and saves Spark dataframes stored on Hive. This data set also handles some incompatible file types such as using partitioned parquet on hive which will not normally allow upserts to existing data without a complete replacement of the existing file/partition.</li><li><code><a href="kedro.extras.datasets.video.VideoDataSet.html" class="internal-link">kedro.extras.datasets.video.VideoDataSet</a></code> - <tt class="rst-docutils literal">VideoDataSet</tt> loads / save video data from a given filepath as sequence of PIL.Image.Image using OpenCV.</li><li><code><a href="kedro.io.AbstractVersionedDataSet.html" class="internal-link">kedro.io.AbstractVersionedDataSet</a></code> - <tt class="rst-docutils literal">AbstractVersionedDataSet</tt> is the base class for all versioned data set implementations. All data sets that implement versioning should extend this abstract class and implement the methods marked as abstract.<ul><li><code><a href="kedro.extras.datasets.email.EmailMessageDataSet.html" class="internal-link">kedro.extras.datasets.email.EmailMessageDataSet</a></code> - <tt class="rst-docutils literal">EmailMessageDataSet</tt> loads/saves an email message from/to a file using an underlying filesystem (e.g.: local, S3, GCS). It uses the <tt class="rst-docutils literal">email</tt> package in the standard library to manage email messages.</li><li><code><a href="kedro.extras.datasets.geopandas.GeoJSONDataSet.html" class="internal-link">kedro.extras.datasets.geopandas.GeoJSONDataSet</a></code> - <tt class="rst-docutils literal">GeoJSONDataSet</tt> loads/saves data to a GeoJSON file using an underlying filesystem (eg: local, S3, GCS). The underlying functionality is supported by geopandas, so it supports all allowed geopandas (pandas) options for loading and saving GeoJSON files.</li><li><code><a href="kedro.extras.datasets.holoviews.HoloviewsWriter.html" class="internal-link">kedro.extras.datasets.holoviews.HoloviewsWriter</a></code> - <tt class="rst-docutils literal">HoloviewsWriter</tt> saves Holoviews objects to image file(s) in an underlying filesystem (e.g. local, S3, GCS).</li><li><code><a href="kedro.extras.datasets.json.JSONDataSet.html" class="internal-link">kedro.extras.datasets.json.JSONDataSet</a></code> - <tt class="rst-docutils literal">JSONDataSet</tt> loads/saves data from/to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). It uses native json to handle the JSON file.<ul><li><code><a href="kedro.extras.datasets.tracking.JSONDataSet.html" class="internal-link">kedro.extras.datasets.tracking.JSONDataSet</a></code> - <tt class="rst-docutils literal">JSONDataSet</tt> saves data to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). It uses native json to handle the JSON file. The <tt class="rst-docutils literal">JSONDataSet</tt> is part of Kedro Experiment Tracking. The dataset is write-only and it is versioned by default.</li><li><code><a href="kedro.extras.datasets.tracking.MetricsDataSet.html" class="internal-link">kedro.extras.datasets.tracking.MetricsDataSet</a></code> - <tt class="rst-docutils literal">MetricsDataSet</tt> saves data to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). It uses native json to handle the JSON file. The <tt class="rst-docutils literal">MetricsDataSet</tt> is part of Kedro Experiment Tracking. The dataset is write-only, it is versioned by default and only takes metrics of numeric values.</li></ul></li><li><code><a href="kedro.extras.datasets.matplotlib.MatplotlibWriter.html" class="internal-link">kedro.extras.datasets.matplotlib.MatplotlibWriter</a></code> - <tt class="rst-docutils literal">MatplotlibWriter</tt> saves one or more Matplotlib objects as image files to an underlying filesystem (e.g. local, S3, GCS).</li><li><code><a href="kedro.extras.datasets.networkx.GMLDataSet.html" class="internal-link">kedro.extras.datasets.networkx.GMLDataSet</a></code> - <tt class="rst-docutils literal">GMLDataSet</tt> loads and saves graphs to a GML file using an underlying filesystem (e.g.: local, S3, GCS). <tt class="rst-docutils literal">NetworkX</tt> is used to create GML data. See <a class="rst-reference external" href="https://networkx.org/documentation/stable/tutorial.html" target="_top">https://networkx.org/documentation/stable/tutorial.html</a> for details.</li><li><code><a href="kedro.extras.datasets.networkx.GraphMLDataSet.html" class="internal-link">kedro.extras.datasets.networkx.GraphMLDataSet</a></code> - <tt class="rst-docutils literal">GraphMLDataSet</tt> loads and saves graphs to a GraphML file using an underlying filesystem (e.g.: local, S3, GCS). <tt class="rst-docutils literal">NetworkX</tt> is used to create GraphML data. See <a class="rst-reference external" href="https://networkx.org/documentation/stable/tutorial.html" target="_top">https://networkx.org/documentation/stable/tutorial.html</a>...</li><li><code><a href="kedro.extras.datasets.networkx.JSONDataSet.html" class="internal-link">kedro.extras.datasets.networkx.JSONDataSet</a></code> - NetworkX <tt class="rst-docutils literal">JSONDataSet</tt> loads and saves graphs to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). <tt class="rst-docutils literal">NetworkX</tt> is used to create JSON data. See <a class="rst-reference external" href="https://networkx.org/documentation/stable/tutorial.html" target="_top">https://networkx.org/documentation/stable/tutorial.html</a>...</li><li><code><a href="kedro.extras.datasets.pandas.CSVDataSet.html" class="internal-link">kedro.extras.datasets.pandas.CSVDataSet</a></code> - <tt class="rst-docutils literal">CSVDataSet</tt> loads/saves data from/to a CSV file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to handle the CSV file.</li><li><code><a href="kedro.extras.datasets.pandas.ExcelDataSet.html" class="internal-link">kedro.extras.datasets.pandas.ExcelDataSet</a></code> - <tt class="rst-docutils literal">ExcelDataSet</tt> loads/saves data from/to a Excel file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to handle the Excel file.</li><li><code><a href="kedro.extras.datasets.pandas.FeatherDataSet.html" class="internal-link">kedro.extras.datasets.pandas.FeatherDataSet</a></code> - <tt class="rst-docutils literal">FeatherDataSet</tt> loads and saves data to a feather file using an underlying filesystem (e.g.: local, S3, GCS). The underlying functionality is supported by pandas, so it supports all allowed pandas options for loading and saving csv files.</li><li><code><a href="kedro.extras.datasets.pandas.GenericDataSet.html" class="internal-link">kedro.extras.datasets.pandas.GenericDataSet</a></code> - <code><a href="kedro.extras.datasets.pandas.GenericDataSet.html" class="internal-link" title="kedro.extras.datasets.pandas.GenericDataSet">pandas.GenericDataSet</a></code> loads/saves data from/to a data file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to dynamically select the appropriate type of read/write target on a best effort basis.</li><li><code><a href="kedro.extras.datasets.pandas.HDFDataSet.html" class="internal-link">kedro.extras.datasets.pandas.HDFDataSet</a></code> - <tt class="rst-docutils literal">HDFDataSet</tt> loads/saves data from/to a hdf file using an underlying filesystem (e.g. local, S3, GCS). It uses pandas.HDFStore to handle the hdf file.</li><li><code><a href="kedro.extras.datasets.pandas.JSONDataSet.html" class="internal-link">kedro.extras.datasets.pandas.JSONDataSet</a></code> - <tt class="rst-docutils literal">JSONDataSet</tt> loads/saves data from/to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to handle the json file.</li><li><code><a href="kedro.extras.datasets.pandas.ParquetDataSet.html" class="internal-link">kedro.extras.datasets.pandas.ParquetDataSet</a></code> - <tt class="rst-docutils literal">ParquetDataSet</tt> loads/saves data from/to a Parquet file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to handle the Parquet file.</li><li><code><a href="kedro.extras.datasets.pandas.XMLDataSet.html" class="internal-link">kedro.extras.datasets.pandas.XMLDataSet</a></code> - <tt class="rst-docutils literal">XMLDataSet</tt> loads/saves data from/to a XML file using an underlying filesystem (e.g.: local, S3, GCS). It uses pandas to handle the XML file.</li><li><code><a href="kedro.extras.datasets.pickle.PickleDataSet.html" class="internal-link">kedro.extras.datasets.pickle.PickleDataSet</a></code> - <tt class="rst-docutils literal">PickleDataSet</tt> loads/saves data from/to a Pickle file using an underlying filesystem (e.g.: local, S3, GCS). The underlying functionality is supported by the specified backend library passed in (defaults to the ...</li><li><code><a href="kedro.extras.datasets.pillow.ImageDataSet.html" class="internal-link">kedro.extras.datasets.pillow.ImageDataSet</a></code> - <tt class="rst-docutils literal">ImageDataSet</tt> loads/saves image data as <code><a href="https://pydocbrowser.github.io/numpy/latest/index.html" class="intersphinx-link">numpy</a></code> from an underlying filesystem (e.g.: local, S3, GCS). It uses Pillow to handle image file.</li><li><code><a href="kedro.extras.datasets.plotly.JSONDataSet.html" class="internal-link">kedro.extras.datasets.plotly.JSONDataSet</a></code> - <tt class="rst-docutils literal">JSONDataSet</tt> loads/saves a plotly figure from/to a JSON file using an underlying filesystem (e.g.: local, S3, GCS).<ul><li><code><a href="kedro.extras.datasets.plotly.PlotlyDataSet.html" class="internal-link">kedro.extras.datasets.plotly.PlotlyDataSet</a></code> - <tt class="rst-docutils literal">PlotlyDataSet</tt> generates a plot from a pandas DataFrame and saves it to a JSON file using an underlying filesystem (e.g.: local, S3, GCS). It loads the JSON into a plotly figure.</li></ul></li><li><code><a href="kedro.extras.datasets.spark.SparkDataSet.html" class="internal-link">kedro.extras.datasets.spark.SparkDataSet</a></code> - <tt class="rst-docutils literal">SparkDataSet</tt> loads and saves Spark dataframes.</li><li><code><a href="kedro.extras.datasets.svmlight.SVMLightDataSet.html" class="internal-link">kedro.extras.datasets.svmlight.SVMLightDataSet</a></code> - <tt class="rst-docutils literal">SVMLightDataSet</tt> loads/saves data from/to a svmlight/libsvm file using an underlying filesystem (e.g.: local, S3, GCS). It uses sklearn functions <tt class="rst-docutils literal">dump_svmlight_file</tt> to save and <tt class="rst-docutils literal">load_svmlight_file</tt> to load a file.</li><li><code><a href="kedro.extras.datasets.tensorflow.TensorFlowModelDataset.html" class="internal-link">kedro.extras.datasets.tensorflow.TensorFlowModelDataset</a></code> - <tt class="rst-docutils literal">TensorflowModelDataset</tt> loads and saves TensorFlow models. The underlying functionality is supported by, and passes input arguments through to, TensorFlow 2.X load_model and save_model methods.</li><li><code><a href="kedro.extras.datasets.text.TextDataSet.html" class="internal-link">kedro.extras.datasets.text.TextDataSet</a></code> - <tt class="rst-docutils literal">TextDataSet</tt> loads/saves data from/to a text file using an underlying filesystem (e.g.: local, S3, GCS)</li><li><code><a href="kedro.extras.datasets.yaml.YAMLDataSet.html" class="internal-link">kedro.extras.datasets.yaml.YAMLDataSet</a></code> - <tt class="rst-docutils literal">YAMLDataSet</tt> loads/saves data from/to a YAML file using an underlying filesystem (e.g.: local, S3, GCS). It uses PyYAML to handle the YAML file.</li></ul></li><li><code><a href="kedro.io.CachedDataSet.html" class="internal-link">kedro.io.CachedDataSet</a></code> - <tt class="rst-docutils literal">CachedDataSet</tt> is a dataset wrapper which caches in memory the data saved, so that the user avoids io operations with slow storage media.</li><li><code><a href="kedro.io.LambdaDataSet.html" class="internal-link">kedro.io.LambdaDataSet</a></code> - <tt class="rst-docutils literal">LambdaDataSet</tt> loads and saves data to a data set. It relies on delegating to specific implementation such as csv, sql, etc.</li><li><code><a href="kedro.io.MemoryDataSet.html" class="internal-link">kedro.io.MemoryDataSet</a></code> - <tt class="rst-docutils literal">MemoryDataSet</tt> loads and saves data from/to an in-memory Python object.</li><li><code><a href="kedro.io.PartitionedDataSet.html" class="internal-link">kedro.io.PartitionedDataSet</a></code> - <tt class="rst-docutils literal">PartitionedDataSet</tt> loads and saves partitioned file-like data using the underlying dataset definition. For filesystem level operations it uses <code>fsspec</code>: <a class="rst-reference external" href="https://github.com/intake/filesystem_spec" target="_top">https://github.com/intake/filesystem_spec</a>.<ul><li><code><a href="kedro.io.IncrementalDataSet.html" class="internal-link">kedro.io.IncrementalDataSet</a></code> - <tt class="rst-docutils literal">IncrementalDataSet</tt> inherits from <tt class="rst-docutils literal">PartitionedDataSet</tt>, which loads and saves partitioned file-like data using the underlying dataset definition. For filesystem level operations it uses <code>fsspec</code>: <a class="rst-reference external" href="https://github.com/intake/filesystem_spec" target="_top">https://github.com/intake/filesystem_spec</a>...</li></ul></li></ul></li></ul></li><li><code>typing.NamedTuple</code><ul><li><a name="kedro.framework.cli.pipeline.PipelineArtifacts"></a><code><a href="kedro.framework.cli.pipeline.PipelineArtifacts.html" class="internal-link">kedro.framework.cli.pipeline.PipelineArtifacts</a></code> - An ordered collection of source_path, tests_path, config_paths</li><li><a name="kedro.framework.startup.ProjectMetadata"></a><code><a href="kedro.framework.startup.ProjectMetadata.html" class="internal-link">kedro.framework.startup.ProjectMetadata</a></code> - Structure holding project metadata derived from <code>pyproject.toml</code></li></ul></li></ul>

    </div>

    <footer class="navbar navbar-default">

  
  <div class="container">
    <a href="index.html">API Documentation</a> for kedro,
  generated by <a href="https://github.com/twisted/pydoctor/">pydoctor</a>
    22.9.1.dev0 at 2023-02-20 22:56:21.
  </div>

  <!-- Search related scripts needs to be loaded at the end of HTML 
    parsing not to introduce overhead and display HTML data asap -->
  <script src="ajax.js" type="text/javascript"></script>
  <script src="searchlib.js" type="text/javascript"></script>
  <script src="search.js" type="text/javascript"></script>

</footer>

    <script src="pydoctor.js" type="text/javascript"></script>

  </body>
</html>